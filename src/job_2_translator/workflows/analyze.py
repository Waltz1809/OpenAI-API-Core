#!/usr/bin/env python3
"""
Analyze Workflow - Ph√¢n t√≠ch ng·ªØ c·∫£nh c·ªßa content
"""

import os
import threading
import queue
import time
from typing import Dict, List

from core.ai_factory import AIClientFactory
from core.yaml_processor import YamlProcessor
from core.logger import Logger


class AnalyzeWorkflow:
    """Workflow ƒë·ªÉ ph√¢n t√≠ch ng·ªØ c·∫£nh."""

    def __init__(self, config: Dict, secret: Dict, input_file: str | None = None, output_base_override: str | None = None):
        self.config = config
        self.secret = secret
        self.processor = YamlProcessor()
        # Setup API client cho context analysis (reuses translate_api settings)
        self.client = AIClientFactory.create_client(config['translate_api'], secret)

        # Load prompt
        self.prompt = self._load_prompt(config['paths']['context_prompt_file'])

        # Validate & set input
        if not input_file:
            raise ValueError("AnalyzeWorkflow now requires explicit input_file (legacy source_yaml_file removed).")
        self.input_file = input_file
        self.base_name = self.processor.get_base_name(self.input_file)

        # Get SDK code from factory (based on translate_api now)
        self.sdk_code = AIClientFactory.get_sdk_code(config['translate_api'])

        # Output base directory override (to maintain parallel directory structure)
        context_subdir = output_base_override or config['paths']['context_dir']

        self.output_file = self.processor.create_output_filename(
            self.input_file,
            context_subdir,
            self.sdk_code,
            "context"
        )

        # Logger (c≈©ng save trong context_subdir)
        self.logger = Logger(
            context_subdir,
            self.base_name,
            self.sdk_code,
            "context"
        )

        print(f"üîß Context SDK: {self.sdk_code.upper()}")
        print(f"ü§ñ Context Model: {self.client.get_model_name()}")
        print(f"üìù Output: {self.output_file}")
        print(f"üìã Log: {self.logger.get_log_path()}")
    
    def _load_prompt(self, prompt_file: str) -> str:
        """Load prompt t·ª´ file."""
        if not os.path.exists(prompt_file):
            raise FileNotFoundError(f"Context prompt file kh√¥ng t·ªìn t·∫°i: {prompt_file}")

        with open(prompt_file, 'r', encoding='utf-8') as f:
            return f.read().strip()
    
    def run(self):
        """Ch·∫°y context analysis workflow."""
        try:
            # 1. Load v√† filter YAML
            print("\nüìñ ƒêang load file YAML...")
            segments = self.processor.load_yaml(self.input_file)
            
            print(f"üìä T·ªïng c·ªông {len(segments)} segments c·∫ßn ph√¢n t√≠ch")
            
            # 2. Ph√¢n t√≠ch ng·ªØ c·∫£nh
            print("\nüîç ƒêang ph√¢n t√≠ch ng·ªØ c·∫£nh...")
            analyzed_segments = self._analyze_segments(segments)
            
            # 3. Save temp file tr∆∞·ªõc
            temp_output_file = os.path.join(
                os.path.dirname(self.output_file), 
                f"temp_{os.path.basename(self.output_file)}"
            )
            print(f"\nüíæ ƒêang save temp file: {os.path.basename(temp_output_file)}...")
            self.processor.save_yaml(analyzed_segments, temp_output_file)
            print(f"‚úÖ K·∫øt qu·∫£ ph√¢n t√≠ch th√¥ l∆∞u t·∫°i: {temp_output_file}")
            
            # 4. Clean t·ª´ temp file -> final file
            print(f"\nüßπ ƒêang clean t·ª´ temp file...")
            self._clean_yaml_file(temp_output_file, self.output_file)
            print(f"‚úÖ Clean ho√†n th√†nh! File cu·ªëi c√πng: {self.output_file}")
            
            # 5. X√≥a temp file
            if os.path.exists(temp_output_file):
                os.remove(temp_output_file)
                print(f"üóëÔ∏è ƒê√£ x√≥a temp file: {os.path.basename(temp_output_file)}")
            
            # 6. Log summary - ƒë·∫øm t·ª´ logger stats
            successful = self.logger.request_count  # S·ªë request th√†nh c√¥ng (c√≥ token_info)
            failed = len(segments) - successful
            self.logger.log_summary(
                len(segments), successful, failed, self.client.get_model_name()
            )
            
            print(f"\nüéâ PH√ÇN T√çCH HO√ÄN TH√ÄNH!")
            print(f"‚úÖ Th√†nh c√¥ng: {successful}/{len(segments)} segments")
            print(f"üìÅ Output: {self.output_file}")
            print(f"üìã Log: {self.logger.get_log_path()}")
            
        except Exception as e:
            print(f"‚ùå L·ªói trong analyze workflow: {e}")
            raise
    
    def _analyze_segments(self, segments: List[Dict]) -> List[Dict]:
        """Ph√¢n t√≠ch ng·ªØ c·∫£nh c·ªßa segments b·∫±ng threading."""
        q = queue.Queue()
        result_dict = {}
        lock = threading.Lock()
        
        # ƒê∆∞a segments v√†o queue
        for idx, segment in enumerate(segments):
            q.put((idx, segment))
            result_dict[idx] = None

        # Threading config
        concurrent_requests = self.config['translate_api'].get('concurrent_requests', 1)
        num_threads = min(concurrent_requests, len(segments))
        threads = []

        print(f"üîß S·ª≠ d·ª•ng {num_threads} threads ƒë·ªìng th·ªùi...")

        # T·∫°o v√† ch·∫°y threads
        for _ in range(num_threads):
            t = threading.Thread(
                target=self._analysis_worker,
                args=(q, result_dict, lock, len(segments))
            )
            t.daemon = True
            t.start()
            threads.append(t)

        # ƒê·ª£i ho√†n th√†nh
        for t in threads:
            t.join()

        # Thu th·∫≠p k·∫øt qu·∫£
        results = []
        for idx in sorted(result_dict.keys()):
            if result_dict[idx] is not None:
                results.append(result_dict[idx])

        return results
    
    def _analysis_worker(self, q: queue.Queue, result_dict: Dict, 
                        lock: threading.Lock, total_segments: int):
        """Worker thread ƒë·ªÉ ph√¢n t√≠ch context."""
        while not q.empty():
            try:
                idx, segment = q.get(block=False)
                segment_id = segment['id']
                
                with lock:
                    processed = len([v for v in result_dict.values() if v is not None])
                    print(f"[{processed + 1}/{total_segments}] üîç {segment_id}")
                
                try:
                    # Ph√¢n t√≠ch context
                    user_prompt = f"Ph√¢n t√≠ch ng·ªØ c·∫£nh c·ªßa ƒëo·∫°n vƒÉn sau:\n\n{segment['content']}"
                    
                    analysis, token_info = self.client.generate_content(
                        self.prompt,
                        user_prompt
                    )
                    
                    # T·∫°o segment m·ªõi v·ªõi analysis
                    analyzed_segment = {
                        'id': segment['id'],
                        'title': segment['title'],
                        'content': analysis  # Replace content v·ªõi analysis
                    }
                    
                    with lock:
                        result_dict[idx] = analyzed_segment
                        self.logger.log_segment(
                            segment_id, "TH√ÄNH C√îNG", token_info=token_info
                        )
                
                except Exception as e:
                    with lock:
                        # Gi·ªØ segment g·ªëc n·∫øu l·ªói
                        result_dict[idx] = segment
                        self.logger.log_segment(
                            segment_id, "TH·∫§T B·∫†I", str(e)
                        )
                
                q.task_done()
                
                # Delay ƒë·ªÉ tr√°nh rate limit (reuse translate_api.delay)
                time.sleep(self.config['translate_api'].get('delay', 1))
                
            except queue.Empty:
                break
    
    def _clean_yaml_file(self, input_file: str, output_file: str):
        """Clean YAML file theo pattern c·ªßa file c≈©: temp -> final."""
        if not self.config['cleaner']['enabled']:
            # N·∫øu kh√¥ng clean, ch·ªâ rename
            os.rename(input_file, output_file)
            return
        
        # ƒê·ªçc temp file
        temp_data = self.processor.load_yaml(input_file)
        
        # Clean t·ª´ng segment
        for segment in temp_data:
            if 'content' in segment and segment['content']:
                segment['content'] = self.processor.clean_content(segment['content'])
        
        # Ghi ra final file
        self.processor.save_yaml(temp_data, output_file)
    
    def _clean_segments(self, segments: List[Dict]):
        """Clean content c·ªßa segments - deprecated, d√πng _clean_yaml_file."""
        if not self.config['cleaner']['enabled']:
            return
        
        for segment in segments:
            if 'content' in segment:
                segment['content'] = self.processor.clean_content(segment['content'])
