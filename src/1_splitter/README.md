# Text Splitter Module

A robust, modular text processing system that intelligently splits large text files into manageable segments for translation pipelines. Features smart boundary detection, progress tracking, resumable processing, and automated cache management.

## ğŸš€ Features

### Core Functionality
- **Smart Text Splitting**: Breaks text at natural boundaries (sentences, paragraphs, words)
- **Markdown Title Extraction**: Extracts `##` headers as titles, excludes them from content
- **Configurable Segments**: Customizable segment length and file type filtering
- **Structure Preservation**: Maintains input directory hierarchy in output
- **YAML Output**: Structured output with unique IDs, titles, and content

### Advanced Features
- **Progress Tracking**: Comprehensive logging with timestamps
- **Resume Capability**: Skip already processed files automatically
- **Change Detection**: Only reprocess files when content changes
- **Cache Management**: Automatic Python cache cleaning utilities
- **Error Handling**: Robust error logging and recovery
- **Modular Architecture**: Clean, maintainable code structure

## ğŸ“ Architecture

### Directory Structure
```
src/1_splitter/
â”œâ”€â”€ main.py              # Main entry point (streamlined)
â”œâ”€â”€ clean_cache.py       # Standalone cache cleaning utility
â”œâ”€â”€ config.yml           # Configuration file
â”œâ”€â”€ README.md            # This documentation
â”œâ”€â”€ ARCHITECTURE.md      # Detailed architecture notes
â””â”€â”€ core/                # Core modules
    â”œâ”€â”€ __init__.py      # Package initialization
    â”œâ”€â”€ config.py        # Configuration management
    â”œâ”€â”€ logging.py       # Logging and tracking
    â”œâ”€â”€ text_processor.py # Text processing and splitting
    â”œâ”€â”€ file_manager.py  # File I/O operations
    â””â”€â”€ cache_manager.py # Python cache management
```

### Module Responsibilities

#### **Main Entry Point (`main.py`)**
- **Size**: ~200 lines (streamlined from 500+)
- **Purpose**: Orchestrates the splitting process
- **Features**: Command-line interface, workflow coordination

#### **Core Modules (`core/`)**

**ConfigManager (`config.py`)**
- Configuration loading and validation
- YAML file parsing with type checking
- Getter methods for each config section

**LogManager (`logging.py`)**
- Session logging with timestamps
- File processing history with MD5 hashing
- Progress tracking and change detection

**TextProcessor (`text_processor.py`)**
- Title extraction from `##` headers
- Smart text segmentation with boundary detection
- Chapter name parsing from filenames

**FileManager (`file_manager.py`)**
- Project root detection and working directory management
- Directory structure creation and file I/O
- YAML output generation

**CacheManager (`cache_manager.py`)**
- Python cache file detection and removal
- Cache size calculation and reporting
- Safe cleanup with error handling

## âš™ï¸ Configuration

The module uses a structured configuration file (`config.yml`):

### Paths Configuration
```yaml
paths:
  input: "0_markdown_exports/Volume_1A"  # Source directory
  output: "input"                        # Destination for segments
  logs: "src/inventory/logs/splitter"    # Log files directory
```

### Processing Configuration
```yaml
processing:
  file_types:                     # Supported file extensions
    - ".md"
    - ".txt"
  segment_length: 10000          # Maximum characters per segment
```

### Logging & Tracking Configuration
```yaml
logging:
  enable_logging: true           # Enable comprehensive logging
  skip_processed: false          # Process all files (true to skip unchanged)
```

## ğŸ¯ Usage

### Basic Text Splitting
```bash
cd src/1_splitter
python main.py
```

### Cache Management
```bash
# Clean Python cache files during splitting
python main.py --clean-cache

# Dry run - see what would be cleaned
python main.py --clean-cache --dry-run

# Standalone cache cleaner (interactive)
python clean_cache.py
```

### What Happens During Processing
1. **Initialization**: Validates config, sets up logging, finds project root
2. **File Discovery**: Scans input directory for supported file types
3. **Change Detection**: Checks processing history, skips unchanged files
4. **Text Processing**: Extracts titles from `##` headers, splits content
5. **Output Generation**: Saves structured YAML segments
6. **Cleanup**: Updates processing log, optionally cleans cache

## ğŸ“„ Output Format

Each input file generates one YAML file containing all its segments:

```yaml
- id: chapter_1_segment_1
  title: "Afterword"  # Extracted from ## header
  content: |
    Okay. If you are here for the first time, welcome.
    If not, I'm glad we could meet again...

- id: chapter_1_segment_2
  title: "Afterword"  # Same title for all segments from same file
  content: |
    As always, I have ended up creating my usual type
    of fairy tale that you can't call science fiction...
```

## ğŸ”§ Technical Details

### Title Processing
- **Extraction**: Finds first `##` header in markdown files
- **Cleaning**: Removes markdown formatting (`**bold**`, `*italic*`)
- **Exclusion**: Title header is NOT included in segment content
- **Fallback**: Uses "Untitled" if no header found

### Smart Text Splitting
The splitter uses a hierarchical approach for optimal break points:
1. **Sentence Boundaries**: `.`, `!`, `?` followed by space/newline
2. **Paragraph Breaks**: Double newlines (`\n\n`)
3. **Line Breaks**: Single newlines as fallback
4. **Word Boundaries**: Last resort splitting at spaces

### Cache Management
- **Detection**: Finds all `__pycache__` directories and `.pyc` files
- **Size Reporting**: Calculates total cache size in human-readable format
- **Safe Removal**: Handles errors gracefully during cleanup
- **Confirmation**: Interactive mode requires user confirmation

### Directory Structure Preservation
```
Input:  0_markdown_exports/Volume_1A/Chapter_01.md
Output: input/Volume_1A/Chapter_01.yml
```

## ğŸ“Š Logging & Tracking

### Session Logs
- **Location**: `{log_path}/splitter_log_YYYYMMDD_HHMMSS.txt`
- **Content**: Timestamped events, errors, statistics
- **Format**: Human-readable with structured timestamps

### Processing History
- **Location**: `{log_path}/processed_files.json`
- **Purpose**: Tracks completed files with content hashes
- **Benefit**: Enables resumable processing and change detection

### Log Output Example
```
[2025-09-05 14:30:15] Starting Text Splitter...
[2025-09-05 14:30:15] Found OpenAI-API-Core directory: d:\Storage\novel\OpenAI-API-Core
[2025-09-05 14:30:15] Changed working directory to: d:\Storage\novel\OpenAI-API-Core
[2025-09-05 14:30:15] Found 45 files to process
[2025-09-05 14:30:16] Processing: Chapter_01.md
[2025-09-05 14:30:16] Processed Chapter_01.md: 3 segments created
[2025-09-05 14:30:16] Saved 3 segments to Chapter_01.yml
[2025-09-05 14:30:17] Python cache size: 2.3 MB
[2025-09-05 14:30:17] Tip: Use --clean-cache to clean Python cache files
```

## ğŸ”§ Customization

### Adding File Types
```yaml
processing:
  file_types:
    - ".md"
    - ".txt"
    - ".rtf"    # Add new types
    - ".docx"
```

### Adjusting Segment Size
```yaml
processing:
  segment_length: 15000  # Increase for larger segments
```

### Disabling Tracking
```yaml
logging:
  enable_logging: false  # Disable all logging
  skip_processed: false  # Process all files every time
```

## ğŸ› ï¸ Error Handling

The module includes comprehensive error handling:
- **Configuration Validation**: Clear messages for missing/invalid config
- **File Access Errors**: Detailed logging with specific file information
- **Cache Cleanup Errors**: Safe handling of permission/access issues
- **Directory Creation**: Automatic creation of missing directories
- **Encoding Issues**: Graceful handling of various text encodings

## ğŸš€ Performance & Benefits

### Modular Design Benefits
- âœ… **Maintainable**: Single responsibility principle, clean interfaces
- âœ… **Testable**: Isolated modules for independent testing
- âœ… **Extensible**: Easy to add new features to specific modules
- âœ… **Reusable**: Core modules can be used in other projects

### Processing Efficiency
- âœ… **Change Detection**: Only processes modified files
- âœ… **Memory Optimized**: Streams large files without full loading
- âœ… **Fast Scanning**: Efficient file system traversal
- âœ… **Cache Management**: Automated cleanup prevents disk bloat

### Data Flow
```
ConfigManager â†’ Load and validate configuration
     â†“
FileManager â†’ Find project root, discover input files
     â†“
LogManager â†’ Check processing history, setup logging
     â†“
TextProcessor â†’ Extract titles, split content into segments
     â†“
FileManager â†’ Save segments to YAML files
     â†“
LogManager â†’ Update processing history, log completion
     â†“
CacheManager â†’ Report cache size, optional cleanup
```

## ğŸ”„ Future Enhancements

The modular structure makes it easy to add:
- **New text processors** for different file formats (PDF, DOCX, HTML)
- **Alternative storage backends** (JSON, XML, database)
- **Advanced logging** with different log levels and filters
- **Configuration hot-reloading** for dynamic settings
- **Plugin system** for custom text processing algorithms
- **Parallel processing** for faster file processing
- **Web interface** for remote operation and monitoring
