#!/usr/bin/env python3
"""
YAML Context Analyzer - Tiá»‡n Ã­ch phÃ¢n tÃ­ch ngá»¯ cáº£nh tá»« file YAML
Äá»c file YAML, gá»­i tá»«ng segment Ä‘áº¿n API Ä‘á»ƒ táº¡o tÃ³m táº¯t ngá»¯ cáº£nh,
sau Ä‘Ã³ ghi káº¿t quáº£ vÃ o má»™t file YAML má»›i.

TÃNH NÄ‚NG:
- Há»— trá»£ cáº£ Gemini vÃ  OpenAI
- TÃ­nh toÃ¡n chi phÃ­ dá»± kiáº¿n
- Thinking budget cho Gemini 2.5 series
- Safety settings cÃ³ thá»ƒ tÃ¹y chá»‰nh
- Token usage tracking

CÃCH DÃ™NG:
1. Chá»‰nh sá»­a config trong yaml_context_analyzer_config.json:
   - "api_provider": "gemini" hoáº·c "openai"
   - "thinking_budget": 0 (táº¯t), 1024 (cá»‘ Ä‘á»‹nh), -1 (dynamic) [chá»‰ Gemini]
   - ÄÆ°á»ng dáº«n source_yaml_file vÃ  system_prompt_file
2. Cháº¡y: python yaml_context_analyzer.py

API PROVIDERS:
- "gemini": Google Gemini API (há»— trá»£ thinking cho 2.5 series)
- "openai": OpenAI API (GPT models)

THINKING BUDGET (chá»‰ Gemini):
- 0: Táº®T thinking (tiáº¿t kiá»‡m token, nhanh hÆ¡n)
- 1024: Báº­t thinking vá»›i budget cá»‘ Ä‘á»‹nh
- -1: Dynamic thinking (model tá»± quyáº¿t Ä‘á»‹nh)

CHI PHÃ:
- Tá»± Ä‘á»™ng tÃ­nh toÃ¡n dá»±a trÃªn pricing data tá»« model_pricing.json
- Hiá»ƒn thá»‹ breakdown chi tiáº¿t input/output/thinking tokens
"""

import os
import sys
import yaml
import json
import threading
import queue
import time
from datetime import datetime

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ import CustomDumper
script_dir = os.path.dirname(os.path.abspath(__file__))
enhanced_dir = os.path.abspath(os.path.join(script_dir, '../enhanced'))
sys.path.append(enhanced_dir)

try:
    from clean_segment import CustomDumper
except ImportError:
    print("Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ import CustomDumper. Sáº½ sá»­ dá»¥ng Dumper máº·c Ä‘á»‹nh cá»§a PyYAML.")
    CustomDumper = yaml.Dumper

# --- SDK Imports (sáº½ import tÃ¹y theo config) ---
# SDK imports sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n trong hÃ m main() tÃ¹y theo config

# --- Pricing Functions ---
def load_pricing_data():
    """Load pricing data tá»« file JSON."""
    pricing_file = os.path.join(enhanced_dir, 'model_pricing.json')
    try:
        with open(pricing_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file pricing: {pricing_file}")
        return None
    except Exception as e:
        print(f"âš ï¸ Lá»—i load pricing data: {e}")
        return None

def calculate_total_cost(token_stats, model_name):
    """TÃ­nh tá»•ng chi phÃ­ dá»±a trÃªn token stats vÃ  model pricing."""
    pricing_data = load_pricing_data()
    if not pricing_data or 'models' not in pricing_data:
        return None
    
    models = pricing_data['models']
    if model_name not in models:
        return None
    
    model_pricing = models[model_name]
    input_tokens = token_stats.get('total_input', 0)
    output_tokens = token_stats.get('total_output', 0)
    thinking_tokens = token_stats.get('total_thinking', 0)
    
    # TÃ­nh chi phÃ­ (giÃ¡ / 1M tokens)
    input_cost = (input_tokens * model_pricing['input_price']) / 1_000_000
    output_cost = (output_tokens * model_pricing['output_price']) / 1_000_000
    thinking_cost = (thinking_tokens * model_pricing['input_price']) / 1_000_000  # Thinking tokens tÃ­nh nhÆ° input
    
    total_cost = input_cost + output_cost + thinking_cost
    
    return {
        'input_cost': input_cost,
        'output_cost': output_cost,
        'thinking_cost': thinking_cost,
        'total_cost': total_cost,
        'currency': pricing_data.get('currency', 'USD')
    }

def supports_thinking_model(model_name):
    """Kiá»ƒm tra xem model cÃ³ há»— trá»£ thinking hay khÃ´ng (chá»‰ 2.5 series)."""
    return any(version in model_name.lower() for version in ['2.5', '2-5'])

# --- CÃ¡c hÃ m vÃ  Class tiá»‡n Ã­ch ---

def load_config(config_path='yaml_context_analyzer_config.json'):
    """Táº£i file cáº¥u hÃ¬nh."""
    full_config_path = os.path.join(script_dir, config_path)
    if not os.path.exists(full_config_path):
        print(f"Lá»—i: File cáº¥u hÃ¬nh '{full_config_path}' khÃ´ng tá»“n táº¡i.")
        return None
    try:
        with open(full_config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Lá»—i khi Ä‘á»c file cáº¥u hÃ¬nh: {e}")
        return None

def load_yaml(file_path):
    """Táº£i file YAML má»™t cÃ¡ch an toÃ n."""
    if not os.path.exists(file_path):
        print(f"Lá»—i: File YAML nguá»“n '{file_path}' khÃ´ng tá»“n táº¡i.")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"Lá»—i khi Ä‘á»c file YAML '{file_path}': {e}")
        return None

def load_prompt(file_path):
    """Táº£i ná»™i dung prompt tá»« file."""
    if not os.path.exists(file_path):
        print(f"Lá»—i: File prompt '{file_path}' khÃ´ng tá»“n táº¡i.")
        return None
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read().strip()

def write_log(log_file, segment_id, status, error=None, token_info=None):
    """Ghi log chi tiáº¿t cho tá»«ng segment."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_message = f"[{timestamp}] {segment_id}: {status}"
    
    # ThÃªm thÃ´ng tin token náº¿u cÃ³
    if token_info:
        input_tokens = token_info.get('input_tokens', 'N/A')
        output_tokens = token_info.get('output_tokens', 'N/A')
        thinking_tokens = token_info.get('thinking_tokens', 'N/A')
        total_tokens = token_info.get('total_tokens', 'N/A')
        
        if thinking_tokens != 'N/A' and thinking_tokens > 0:
            log_message += f" | Tokens: Input={input_tokens}, Output={output_tokens}, Thinking={thinking_tokens}, Total={total_tokens}"
        else:
            log_message += f" | Tokens: Input={input_tokens}, Output={output_tokens}, Total={total_tokens}"
    
    if error:
        log_message += f" - Lá»—i: {error}"
    
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(log_message + "\n")
    print(log_message)

# --- Logic PhÃ¢n TÃ­ch ---

def analysis_worker_gemini(q, result_dict, client, model_name, generation_config, system_prompt, lock, total_segments, token_stats, delay, log_file):
    """Worker cho thread phÃ¢n tÃ­ch segment sá»­ dá»¥ng Gemini API."""
    while not q.empty():
        try:
            index, segment = q.get(block=False)
            segment_id = segment.get('id', 'Unknown_ID')
            original_title = segment.get('title', '')
            content = segment.get('content', '')

            with lock:
                processed_count = len(result_dict)
                print(f"[{processed_count + 1}/{total_segments}] Äang phÃ¢n tÃ­ch: {segment_id}...")

            if not content.strip():
                with lock:
                    print(f"Cáº£nh bÃ¡o: Bá» qua segment {segment_id} vÃ¬ khÃ´ng cÃ³ ná»™i dung.")
                result_dict[index] = {'id': segment_id, 'title': original_title, 'content': ''}
                q.task_done()
                continue

            try:
                # Táº¡o prompt hoÃ n chá»‰nh cho Gemini
                full_prompt = f"{system_prompt}\n\nDÆ°á»›i Ä‘Ã¢y lÃ  ná»™i dung cáº§n tÃ³m táº¯t:\n\n---\n\n{content}"
                
                response = client.models.generate_content(
                    model=model_name,
                    contents=full_prompt,
                    config=generation_config
                )
                
                # Kiá»ƒm tra xem prompt cÃ³ bá»‹ cháº·n khÃ´ng
                if not response.candidates:
                    block_reason = "KhÃ´ng rÃµ"
                    if response.prompt_feedback:
                        block_reason = response.prompt_feedback.block_reason.name
                    raise Exception(f"Prompt bá»‹ cháº·n bá»Ÿi bá»™ lá»c an toÃ n: {block_reason}")

                # Láº¥y token usage (Native SDK)
                token_info = None
                try:
                    if hasattr(response, 'usage_metadata') and response.usage_metadata:
                        input_tokens = response.usage_metadata.prompt_token_count
                        output_tokens = response.usage_metadata.candidates_token_count
                        total_tokens = response.usage_metadata.total_token_count
                        
                        # Láº¥y thinking tokens náº¿u model há»— trá»£
                        thinking_tokens = 0
                        if supports_thinking_model(model_name):
                            thinking_tokens = getattr(response.usage_metadata, 'thoughts_token_count', 0) or 0
                        
                        token_info = {
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'thinking_tokens': thinking_tokens,
                            'total_tokens': total_tokens
                        }
                        with lock:
                            token_stats['total_input'] += input_tokens
                            token_stats['total_output'] += output_tokens
                            token_stats['total_thinking'] += thinking_tokens
                            token_stats['total_overall'] += total_tokens
                            token_stats['request_count'] += 1
                except AttributeError:
                    pass
                
                summary_text = response.text
                if not summary_text or not summary_text.strip():
                    raise Exception("Model tráº£ vá» content trá»‘ng - cÃ³ thá»ƒ do thinking budget quÃ¡ cao hoáº·c lá»—i API")
                
                new_segment = {
                    'id': segment_id,
                    'title': original_title,
                    'content': summary_text.strip()
                }
                with lock:
                    result_dict[index] = new_segment
                    write_log(log_file, segment_id, "THÃ€NH CÃ”NG", token_info=token_info)

            except Exception as e:
                with lock:
                    result_dict[index] = segment # Giá»¯ láº¡i segment gá»‘c
                    write_log(log_file, segment_id, "THáº¤T Báº I", str(e))
            
            q.task_done()
            time.sleep(delay)

        except queue.Empty:
            break

def analysis_worker_openai(q, result_dict, client, model_name, system_prompt, lock, total_segments, token_stats, delay, log_file, temperature, max_tokens):
    """Worker cho thread phÃ¢n tÃ­ch segment sá»­ dá»¥ng OpenAI API."""
    while not q.empty():
        try:
            index, segment = q.get(block=False)
            segment_id = segment.get('id', 'Unknown_ID')
            original_title = segment.get('title', '')
            content = segment.get('content', '')

            with lock:
                processed_count = len(result_dict)
                print(f"[{processed_count + 1}/{total_segments}] Äang phÃ¢n tÃ­ch: {segment_id}...")

            if not content.strip():
                with lock:
                    print(f"Cáº£nh bÃ¡o: Bá» qua segment {segment_id} vÃ¬ khÃ´ng cÃ³ ná»™i dung.")
                result_dict[index] = {'id': segment_id, 'title': original_title, 'content': ''}
                q.task_done()
                continue

            try:
                # Táº¡o prompt cho OpenAI
                full_prompt = f"{system_prompt}\n\nDÆ°á»›i Ä‘Ã¢y lÃ  ná»™i dung cáº§n tÃ³m táº¯t:\n\n---\n\n{content}"
                
                response = client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"DÆ°á»›i Ä‘Ã¢y lÃ  ná»™i dung cáº§n tÃ³m táº¯t:\n\n---\n\n{content}"}
                    ],
                    model=model_name,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                # Láº¥y token usage (OpenAI)
                token_info = None
                try:
                    if hasattr(response, 'usage') and response.usage:
                        input_tokens = response.usage.prompt_tokens
                        output_tokens = response.usage.completion_tokens
                        total_tokens = response.usage.total_tokens
                        
                        token_info = {
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'thinking_tokens': 0,  # OpenAI khÃ´ng cÃ³ thinking tokens
                            'total_tokens': total_tokens
                        }
                        with lock:
                            token_stats['total_input'] += input_tokens
                            token_stats['total_output'] += output_tokens
                            token_stats['total_thinking'] += 0
                            token_stats['total_overall'] += total_tokens
                            token_stats['request_count'] += 1
                except AttributeError:
                    pass
                
                summary_text = response.choices[0].message.content
                if not summary_text or not summary_text.strip():
                    raise Exception("Model tráº£ vá» content trá»‘ng")
                
                new_segment = {
                    'id': segment_id,
                    'title': original_title,
                    'content': summary_text.strip()
                }
                with lock:
                    result_dict[index] = new_segment
                    write_log(log_file, segment_id, "THÃ€NH CÃ”NG", token_info=token_info)

            except Exception as e:
                with lock:
                    result_dict[index] = segment # Giá»¯ láº¡i segment gá»‘c
                    write_log(log_file, segment_id, "THáº¤T Báº I", str(e))
            
            q.task_done()
            time.sleep(delay)

        except queue.Empty:
            break

def analyze_and_summarize_threaded(segments, sdk_components, system_prompt, config, log_file):
    """Xá»­ lÃ½ tÃ³m táº¯t cÃ¡c segment báº±ng threading vá»›i SDK linh Ä‘á»™ng."""
    q = queue.Queue()
    # DÃ¹ng dict Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»± cá»§a cÃ¡c segment Ä‘Æ°á»£c giá»¯ nguyÃªn
    result_dict = {}
    lock = threading.Lock()
    total_segments = len(segments)
    
    # Khá»Ÿi táº¡o token statistics
    token_stats = {
        'total_input': 0,
        'total_output': 0,
        'total_thinking': 0,
        'total_overall': 0,
        'request_count': 0
    }
    
    for i, seg in enumerate(segments):
        q.put((i, seg))

    api_config = config['api_settings']
    num_threads = min(api_config.get("concurrent_requests", 5), total_segments)
    threads = []
    
    print(f"\nBáº¯t Ä‘áº§u tÃ³m táº¯t {total_segments} segment vá»›i {num_threads} threads...")
    
    # TÃ¹y theo API provider Ä‘á»ƒ gá»i worker function khÃ¡c nhau
    api_provider = api_config.get('api_provider', 'gemini')
    delay = api_config.get('delay', 1)
    
    if api_provider == 'openai':
        # OpenAI API
        client = sdk_components['client']
        model_name = sdk_components['model_name']
        temperature = sdk_components['temperature']
        max_tokens = sdk_components['max_tokens']
        
        for _ in range(num_threads):
            t = threading.Thread(
                target=analysis_worker_openai,
                args=(
                    q, result_dict, client, model_name, system_prompt,
                    lock, total_segments, token_stats, delay, log_file, temperature, max_tokens
                )
            )
            t.daemon = True
            t.start()
            threads.append(t)
    else:
        # Gemini API
        client = sdk_components['client']
        model_name = sdk_components['model_name']
        generation_config = sdk_components['generation_config']
        
        for _ in range(num_threads):
            t = threading.Thread(
                target=analysis_worker_gemini,
                args=(
                    q, result_dict, client, model_name, generation_config, system_prompt,
                    lock, total_segments, token_stats, delay, log_file
                )
            )
            t.daemon = True
            t.start()
            threads.append(t)
    
    q.join() # Chá» queue Ä‘Æ°á»£c xá»­ lÃ½ háº¿t
    
    # Chuyá»ƒn Ä‘á»•i dict káº¿t quáº£ thÃ nh list theo Ä‘Ãºng thá»© tá»±
    final_results = [result_dict[i] for i in sorted(result_dict.keys())]
    return final_results, token_stats

def save_output_yaml(data, output_dir, base_filename):
    """LÆ°u danh sÃ¡ch cÃ¡c segment Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ ra file YAML."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    output_file_path = os.path.join(output_dir, f"{base_filename}_context.yaml")
    
    try:
        with open(output_file_path, 'w', encoding='utf-8') as f:
            yaml.dump(data, f, allow_unicode=True, sort_keys=False, Dumper=CustomDumper)
        print(f"âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o file YAML: {output_file_path}")
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file YAML: {e}")


# --- Main Workflow ---
def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y script."""
    print("="*60)
    print("      YAML CONTEXT ANALYZER - TIá»†N ÃCH PHÃ‚N TÃCH NGá»® Cáº¢NH")
    print("="*60)

    # 1. Táº£i config
    config_path = 'yaml_context_analyzer_config.json'
    config = load_config(config_path)
    if not config:
        sys.exit(1)
    
    api_config = config['api_settings']
    paths_config = config['paths']

    # 2. Láº¥y thÃ´ng tin tá»« config
    yaml_file = paths_config.get("source_yaml_file")
    if not yaml_file or "ÄÆ¯á»œNG_DáºªN" in yaml_file:
        print(f"Lá»—i: Vui lÃ²ng chá»‰ Ä‘á»‹nh 'source_yaml_file' há»£p lá»‡ trong file '{config_path}'.")
        sys.exit(1)
        
    segments = load_yaml(yaml_file)
    if not segments:
        sys.exit(1)

    prompt_file = paths_config.get("system_prompt_file")
    if not prompt_file or not os.path.exists(prompt_file):
        print(f"Lá»—i: Vui lÃ²ng chá»‰ Ä‘á»‹nh 'system_prompt_file' há»£p lá»‡ trong file '{config_path}'.")
        sys.exit(1)

    system_prompt = load_prompt(prompt_file)
    if not system_prompt:
        sys.exit(1)

    # 3. Kiá»ƒm tra API key
    if "YOUR_API_KEY" in api_config.get("api_key", ""):
        api_provider = api_config.get('api_provider', 'gemini')
        api_key = input(f"Vui lÃ²ng nháº­p {api_provider.upper()} API Key cá»§a báº¡n: ").strip()
        if not api_key:
            print("API key lÃ  báº¯t buá»™c. Dá»«ng chÆ°Æ¡ng trÃ¬nh.")
            sys.exit(1)
        api_config['api_key'] = api_key
    
    print(f"\nBáº¯t Ä‘áº§u phÃ¢n tÃ­ch file: {yaml_file}")
    print(f"Sá»­ dá»¥ng system prompt: {prompt_file}")
    
    # 4. Khá»Ÿi táº¡o API client tÃ¹y theo config
    api_provider = api_config.get('api_provider', 'gemini')
    model_name = api_config["model"]
    print(f"ğŸ”§ Sá»­ dá»¥ng API: {api_provider}")
    
    sdk_components = {}
    
    try:
        if api_provider == 'openai':
            # Import OpenAI SDK
            import openai
            
            client = openai.OpenAI(
                api_key=api_config["api_key"],
                base_url=api_config.get("base_url", "https://api.openai.com/v1")
            )
            
            sdk_components = {
                'client': client,
                'model_name': model_name,
                'temperature': api_config["temperature"],
                'max_tokens': api_config.get("max_tokens", 4000)
            }
            
            print(f"âœ… (OpenAI) ÄÃ£ khá»Ÿi táº¡o thÃ nh cÃ´ng model: {model_name}")
            
        else:
            # Import Gemini Native SDK
            from google import genai
            from google.genai import types
            
            client = genai.Client(api_key=api_config["api_key"])
            
            # Cáº¥u hÃ¬nh an toÃ n - Táº®T Táº¤T Cáº¢
            safety_settings = [
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, 
                    threshold=types.HarmBlockThreshold.OFF
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, 
                    threshold=types.HarmBlockThreshold.OFF
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, 
                    threshold=types.HarmBlockThreshold.OFF
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, 
                    threshold=types.HarmBlockThreshold.OFF
                ),
            ]
            
            # Cáº¥u hÃ¬nh sinh ná»™i dung
            generation_config_params = {
                "temperature": api_config["temperature"],
                "max_output_tokens": api_config.get("max_tokens", 4000)
            }
            
            # Kiá»ƒm tra xem model cÃ³ há»— trá»£ thinking khÃ´ng
            if supports_thinking_model(model_name):
                thinking_budget = api_config.get('thinking_budget', 0)  # Máº·c Ä‘á»‹nh = 0
                if thinking_budget is not None:
                    try:
                        generation_config_params['thinking_config'] = types.ThinkingConfig(
                            thinking_budget=int(thinking_budget)
                        )
                        if thinking_budget == 0:
                            print(f"ğŸ’¡ ÄÃ£ Táº®T thinking (thinking_budget = 0) Ä‘á»ƒ tiáº¿t kiá»‡m token")
                        else:
                            print(f"ğŸ’¡ ÄÃ£ Ã¡p dá»¥ng thinking_budget: {thinking_budget}")
                    except Exception as e:
                        print(f"âš ï¸ Lá»—i khi Ã¡p dá»¥ng thinking_budget: {e}. Bá» qua...")
            else:
                print(f"â„¹ï¸ Model {model_name} khÃ´ng há»— trá»£ thinking - bá» qua thinking_config")

            generation_config = types.GenerateContentConfig(**generation_config_params, safety_settings=safety_settings)
            
            sdk_components = {
                'client': client,
                'model_name': model_name,
                'generation_config': generation_config
            }
            
            print(f"âœ… (Gemini) ÄÃ£ khá»Ÿi táº¡o thÃ nh cÃ´ng model: {model_name}")
            
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o API client: {e}")
        sys.exit(1)

    # 5. Táº¡o log file vÃ  thá»±c thi phÃ¢n tÃ­ch
    base_filename = os.path.splitext(os.path.basename(yaml_file))[0]
    output_dir = paths_config.get("output_dir", "output")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    log_file = os.path.join(output_dir, f"{base_filename}_context.txt")
    
    # Ghi log ban Ä‘áº§u
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"--- Báº®T Äáº¦U YAML CONTEXT ANALYZER {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
        f.write(f"File nguá»“n: {yaml_file}\n")
        f.write(f"Model: {model_name}\n")
        f.write(f"API Provider: {api_provider}\n")
        f.write(f"Tá»•ng segments: {len(segments)}\n\n")
    
    summarized_segments, token_stats = analyze_and_summarize_threaded(segments, sdk_components, system_prompt, config, log_file)

    if not summarized_segments:
        print("\nKhÃ´ng cÃ³ káº¿t quáº£ nÃ o Ä‘Æ°á»£c xá»­ lÃ½. Dá»«ng chÆ°Æ¡ng trÃ¬nh.")
        sys.exit(1)
        
    # 6. LÆ°u káº¿t quáº£
    save_output_yaml(summarized_segments, output_dir, base_filename)
    
    # 7. Ghi token summary vÃ o log
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n--- TOKEN USAGE SUMMARY ---\n")
        f.write(f"Tá»•ng sá»‘ request thÃ nh cÃ´ng: {token_stats['request_count']}\n")
        f.write(f"Tá»•ng Input tokens: {token_stats['total_input']:,}\n")
        f.write(f"Tá»•ng Output tokens: {token_stats['total_output']:,}\n")
        if api_provider == 'gemini' and supports_thinking_model(model_name):
            f.write(f"Tá»•ng Thinking tokens: {token_stats['total_thinking']:,}\n")
        f.write(f"Tá»•ng tokens sá»­ dá»¥ng: {token_stats['total_overall']:,}\n")
        if token_stats['request_count'] > 0:
            avg_input = token_stats['total_input'] / token_stats['request_count']
            avg_output = token_stats['total_output'] / token_stats['request_count']
            f.write(f"Trung bÃ¬nh Input tokens/request: {avg_input:.1f}\n")
            f.write(f"Trung bÃ¬nh Output tokens/request: {avg_output:.1f}\n")
            if api_provider == 'gemini' and supports_thinking_model(model_name):
                avg_thinking = token_stats['total_thinking'] / token_stats['request_count']
                f.write(f"Trung bÃ¬nh Thinking tokens/request: {avg_thinking:.1f}\n")
        
        # Ghi thÃ´ng tin chi phÃ­ vÃ o log
        f.write(f"\n--- CHI PHÃ Dá»° KIáº¾N ---\n")
        cost_info = calculate_total_cost(token_stats, model_name)
        if cost_info:
            f.write(f"Chi phÃ­ Input tokens: ${cost_info['input_cost']:.6f}\n")
            f.write(f"Chi phÃ­ Output tokens: ${cost_info['output_cost']:.6f}\n")
            if cost_info['thinking_cost'] > 0:
                f.write(f"Chi phÃ­ Thinking tokens: ${cost_info['thinking_cost']:.6f}\n")
            f.write(f"Tá»”NG CHI PHÃ Dá»° KIáº¾N: ${cost_info['total_cost']:.6f} {cost_info['currency']}\n")
        else:
            f.write(f"KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin giÃ¡ cáº£ cho model: {model_name}\n")
        
        f.write(f"\n--- Káº¾T THÃšC ANALYZER {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
    
    # 8. Hiá»ƒn thá»‹ chi phÃ­
    cost_info = calculate_total_cost(token_stats, model_name)
    
    print("\nğŸ“Š TOKEN USAGE SUMMARY:")
    print(f"â”œâ”€ Sá»‘ request thÃ nh cÃ´ng: {token_stats['request_count']}")
    print(f"â”œâ”€ Tá»•ng Input tokens: {token_stats['total_input']:,}")
    print(f"â”œâ”€ Tá»•ng Output tokens: {token_stats['total_output']:,}")
    if api_provider == 'gemini' and supports_thinking_model(model_name):
        print(f"â”œâ”€ Tá»•ng Thinking tokens: {token_stats['total_thinking']:,}")
    print(f"â””â”€ Tá»•ng tokens sá»­ dá»¥ng: {token_stats['total_overall']:,}")
    
    if token_stats['request_count'] > 0:
        avg_input = token_stats['total_input'] / token_stats['request_count']
        avg_output = token_stats['total_output'] / token_stats['request_count']
        if api_provider == 'gemini' and supports_thinking_model(model_name) and token_stats['total_thinking'] > 0:
            avg_thinking = token_stats['total_thinking'] / token_stats['request_count']
            print(f"   Trung bÃ¬nh: {avg_input:.1f} input + {avg_output:.1f} output + {avg_thinking:.1f} thinking = {avg_input + avg_output + avg_thinking:.1f} tokens/request")
        else:
            print(f"   Trung bÃ¬nh: {avg_input:.1f} input + {avg_output:.1f} output = {avg_input + avg_output:.1f} tokens/request")
    
    # Hiá»ƒn thá»‹ thÃ´ng tin chi phÃ­
    print(f"\nğŸ’° CHI PHÃ Dá»° KIáº¾N:")
    if cost_info:
        print(f"â”œâ”€ Chi phÃ­ Input: ${cost_info['input_cost']:.6f}")
        print(f"â”œâ”€ Chi phÃ­ Output: ${cost_info['output_cost']:.6f}")
        if cost_info['thinking_cost'] > 0:
            print(f"â”œâ”€ Chi phÃ­ Thinking: ${cost_info['thinking_cost']:.6f}")
        print(f"â””â”€ ğŸ’µ Tá»”NG: ${cost_info['total_cost']:.6f} {cost_info['currency']}")
    else:
        print(f"â””â”€ âš ï¸ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin giÃ¡ cáº£ cho model: {model_name}")
    
    print("\nğŸ‰ PhÃ¢n tÃ­ch hoÃ n táº¥t!")
    print(f"ğŸ“„ Log chi tiáº¿t: {log_file}")

if __name__ == "__main__":
    main() 