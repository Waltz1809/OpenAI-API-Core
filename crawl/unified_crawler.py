#!/usr/bin/env python3
"""
Unified Novel Crawler
====================

Crawler vá»›i config JSON, retry mechanism vÃ  single TXT output
Há»— trá»£ crawl nhiá»u series tá»« tw.linovelib.com
"""

import os
import json
import time
import re
import logging
import yaml
from datetime import datetime
from playwright.sync_api import sync_playwright
from parsers.tw_parser import TWLinovelibParser
from parsers.hjwzw_parser import HjwzwParser
from parsers.zhswx_parser import ZhswxParser
from parsers.dxmwx_parser import DxmwxParser
from parsers.shuba_parser import ShubaParser
from parsers.czbooks_parser import CZBooksParser
from parsers.piaotia_parser import PiaotiaParser
from parsers.quanben_parser import QuanbenParser
from parsers.sto55_parser import Sto55Parser
from chapter_detection import enhance_chapter_detection

from clean_logger import CleanLogger, PiaotiaLogger
import sys


class UnifiedCrawler:
    """Main crawler vá»›i config vÃ  retry mechanism"""
    
    def __init__(self, config_file="config.json"):
        """
        Args:
            config_file: ÄÆ°á»ng dáº«n Ä‘áº¿n file config JSON
        """
        self.config_file = config_file
        self.config = self.load_config()
        self.settings = self.config.get('settings', {})
        
        # Browser instances
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        
        # Stats
        self.error_count = 0
        self.restart_threshold = self.settings.get('browser_restart_after_errors', 5)
        self.current_parser = None  # Parser hiá»‡n táº¡i cho series
        

        
        # Setup logging
        self.setup_logging()
        
    def setup_logging(self):
        """Thiáº¿t láº­p logging system"""
        log_dir = self.settings.get('log_dir', 'logs')
        if not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"crawler_{timestamp}.log")
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()  # Console output
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸš€ Crawler khá»Ÿi Ä‘á»™ng")
        self.logger.info(f"ğŸ“‹ Config file: {self.config_file}")
        print(f"ğŸ“ Log file: {log_file}")
    
    def load_config(self):
        """Load config tá»« JSON file"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ Lá»—i load config: {e}")
            return {"series": [], "settings": {}}
    
    def start_browser(self):
        """Khá»Ÿi Ä‘á»™ng browser vá»›i anti-detection"""
        try:
            if self.playwright:
                self.close_browser()
            
            self.playwright = sync_playwright().start()
            
            # Browser engine: láº¥y tá»« config hoáº·c máº·c Ä‘á»‹nh lÃ  chromium
            browser_type = self.settings.get('browser', 'chromium').lower()
            headless_mode = self.settings.get('headless', True)
            print(f"ğŸŒ Khá»Ÿi Ä‘á»™ng browser: {browser_type.title()}")
            print(f"ğŸ‘ï¸  Headless mode: {'Báº­t' if headless_mode else 'Táº®T (Debug mode)'}")
            
            # Chá»n browser engine
            if browser_type == 'edge':
                self.browser = self.playwright.chromium.launch(
                    headless=headless_mode,
                    channel='msedge',  # Sá»­ dá»¥ng Microsoft Edge
                    args=[
                        '--no-sandbox',
                        '--disable-blink-features=AutomationControlled',
                        '--disable-web-security'
                    ]
                )
            elif browser_type == 'firefox':
                self.browser = self.playwright.firefox.launch(
                    headless=headless_mode,
                    args=['--no-sandbox']
                )
            elif browser_type == 'webkit':
                self.browser = self.playwright.webkit.launch(
                    headless=headless_mode,
                    args=['--no-sandbox']
                )
            else:  # chromium (máº·c Ä‘á»‹nh)
                self.browser = self.playwright.chromium.launch(
                    headless=headless_mode,
                    args=[
                        '--no-sandbox',
                        '--disable-blink-features=AutomationControlled',
                        '--disable-web-security'
                    ]
                )
            
            self.context = self.browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            self.page = self.context.new_page()
            self.page.set_default_timeout(self.settings.get('timeout', 30000))
            
            # Anti-detection script
            self.page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                
                window.chrome = {
                    runtime: {},
                    loadTimes: function() {},
                    csi: function() {},
                    app: {}
                };
            """)
            
            print("ğŸŒ Browser Ä‘Ã£ khá»Ÿi Ä‘á»™ng")
            return True
            
        except Exception as e:
            print(f"âŒ Lá»—i khá»Ÿi Ä‘á»™ng browser: {e}")
            return False
    
    def close_browser(self):
        """ÄÃ³ng browser vÃ  cleanup"""
        try:
            if self.page:
                self.page.close()
            if self.context:
                self.context.close()
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
            
            self.page = None
            self.context = None
            self.browser = None
            self.playwright = None
            
        except Exception as e:
            print(f"âš ï¸  Lá»—i Ä‘Ã³ng browser: {e}")
    

    
    def get_parser(self, url):
        """Chá»n parser phÃ¹ há»£p dá»±a trÃªn URL"""
        if "hjwzw.com" in url:
            return HjwzwParser
        elif "linovelib.com" in url:
            return TWLinovelibParser
        elif "zhswx.com" in url:
            return ZhswxParser
        elif "dxmwx.org" in url or "dxmwx.com" in url:
            return DxmwxParser
        elif "czbooks.net" in url:
            return CZBooksParser
        elif "piaotia.com" in url:
            return PiaotiaParser
        elif "quanben.io" in url:
            return QuanbenParser
        elif "sto55.com" in url:
            return Sto55Parser
        elif any(domain in url for domain in ["69shuba.com", "69shu.com", "69xinshu.com", "69shu.pro", "69shuba.pro"]):
            # Æ¯u tiÃªn sá»­ dá»¥ng requests parser (trÃ¡nh timeout vá»›i Playwright)
            # Há»— trá»£ táº¥t cáº£ domains: 69shuba.com, 69shu.com, 69xinshu.com, 69shu.pro, 69shuba.pro
            return ShubaParser
        else:
            print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y parser cho URL: {url}")
            return None

    def get_parser_by_type(self, parser_type):
        """Chá»n parser dá»±a trÃªn type string"""
        parser_map = {
            'hjwzw': HjwzwParser,
            'tw': TWLinovelibParser,
            'linovelib': TWLinovelibParser,
            'zhswx': ZhswxParser,
            'dxmwx': DxmwxParser,
            'czbooks': CZBooksParser,
            'piaotia': PiaotiaParser,
            'quanben': QuanbenParser,
            'shuba': ShubaParser,
            '69shuba': ShubaParser,
            'sto55': Sto55Parser
        }

        parser_cls = parser_map.get(parser_type.lower())
        if not parser_cls:
            print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y parser cho type: {parser_type}")
            print(f"ğŸ“‹ Available types: {list(parser_map.keys())}")
        return parser_cls
    
    def crawl_with_retry(self, url):
        """
        Crawl má»™t URL vá»›i retry mechanism
        
        Returns:
            dict hoáº·c None náº¿u fail
        """
        max_retries = self.settings.get('max_retries', 3)
        retry_delay = self.settings.get('retry_delay', 10)
        
        for attempt in range(max_retries + 1):
            try:
                # Navigate to page
                print(f"ğŸ“– Crawl attempt {attempt + 1}: {url}")
                self.logger.info(f"ğŸ“– Crawl attempt {attempt + 1}: {url}")
                
                if not self.page:
                    raise Exception("Browser page khÃ´ng kháº£ dá»¥ng")
                
                self.page.goto(url, wait_until='networkidle')
                time.sleep(2)  # Äá»£i content load
                
                # Sá»­ dá»¥ng parser tá»« series config (Ä‘Ã£ Ä‘Æ°á»£c set á»Ÿ run_all_series)
                parser = self.current_parser
                if not parser:
                    raise Exception("KhÃ´ng cÃ³ parser Ä‘Æ°á»£c set")
                
                result = parser.extract_content(self.page, url)
                
                if result['success']:
                    self.error_count = 0  # Reset error count
                    result['original_url'] = url # LÆ°u láº¡i URL gá»‘c
                    self.logger.info(f"âœ… Crawl thÃ nh cÃ´ng: {result.get('title', 'No title')}")
                    return result
                else:
                    raise Exception("Failed to extract content")
                    
            except Exception as e:
                self.error_count += 1
                error_msg = f"âš ï¸  Attempt {attempt + 1} failed: {e}"
                print(error_msg)
                self.logger.warning(error_msg)
                
                # Restart browser náº¿u quÃ¡ nhiá»u lá»—i
                if self.error_count >= self.restart_threshold:
                    restart_msg = "ğŸ”„ Restart browser do quÃ¡ nhiá»u lá»—i..."
                    print(restart_msg)
                    self.logger.warning(restart_msg)
                    self.start_browser()
                    self.error_count = 0
                
                if attempt < max_retries:
                    retry_msg = f"â³ Retry sau {retry_delay} giÃ¢y..."
                    print(retry_msg)
                    self.logger.info(retry_msg)
                    time.sleep(retry_delay)
                else:
                    fail_msg = f"ğŸ’¥ Tháº¥t báº¡i hoÃ n toÃ n sau {max_retries + 1} attempts"
                    print(fail_msg)
                    self.logger.error(fail_msg)
                    return None
    
    def run_all_series(self):
        """Cháº¡y crawl cho táº¥t cáº£ series trong config"""
        if not self.start_browser():
            print("âŒ KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng browser")
            return

        try:
            series_list = self.config.get('series', [])
            enabled_series = [s for s in series_list if s.get('enabled', True)]

            print(f"ğŸš€ Sáº½ crawl {len(enabled_series)} series")
            self.logger.info(f"ğŸš€ Sáº½ crawl {len(enabled_series)} series")

            for i, series in enumerate(enabled_series):
                print(f"\n{'='*60}")
                print(f"Series {i+1}/{len(enabled_series)}: {series['name']}")
                print('='*60)

                self.logger.info(f"ğŸ“š Báº¯t Ä‘áº§u series {i+1}/{len(enabled_series)}: {series['name']}")

                output_file = series.get('output_file', f"{series['name']}.txt")

                # Äáº£m báº£o thÆ° má»¥c tá»“n táº¡i
                output_dir = os.path.dirname(output_file)
                if output_dir and not os.path.exists(output_dir):
                    os.makedirs(output_dir, exist_ok=True)
                    print(f"ğŸ“ Táº¡o thÆ° má»¥c: {output_dir}")
                    self.logger.info(f"ğŸ“ Táº¡o thÆ° má»¥c: {output_dir}")

                # JSON-only approach: táº¥t cáº£ parsers Ä‘á»u dÃ¹ng JSON mapping
                json_mapping = series.get('json_mapping')
                parser_type = series.get('parser', '')

                if not json_mapping:
                    print("âŒ Thiáº¿u json_mapping trong series config")
                    self.logger.error("âŒ Thiáº¿u json_mapping trong series config")
                    continue

                if not parser_type:
                    print("âŒ Thiáº¿u parser type trong series config")
                    self.logger.error("âŒ Thiáº¿u parser type trong series config")
                    continue

                print(f"ğŸ“‹ Sá»­ dá»¥ng JSON mapping cho parser {parser_type}: {json_mapping}")
                self.logger.info(f"ğŸ“‹ Sá»­ dá»¥ng JSON mapping cho parser {parser_type}: {json_mapping}")

                parser_cls = self.get_parser_by_type(parser_type)
                if not parser_cls:
                    print(f"âŒ KhÃ´ng tÃ¬m tháº¥y parser cho type: {parser_type}")
                    self.logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y parser cho type: {parser_type}")
                    continue
                
                # Set parser cho series nÃ y
                self.current_parser = parser_cls

                # Táº¡o instance cá»§a parser Ä‘á»ƒ gá»i method
                parser_instance = parser_cls()

                # JSON-only: chá»‰ dÃ¹ng get_catalog_links_from_config
                enhanced_method = getattr(parser_instance, 'get_catalog_links_from_config', None)
                if enhanced_method and callable(enhanced_method):
                    links = enhanced_method(self.page, "", series)  # catalog_url khÃ´ng cáº§n thiáº¿t
                else:
                    print("âŒ Parser khÃ´ng cÃ³ method get_catalog_links_from_config")
                    self.logger.error("âŒ Parser khÃ´ng cÃ³ method get_catalog_links_from_config")
                    continue
                if not links:
                    print("âŒ KhÃ´ng tÃ¬m tháº¥y link chÆ°Æ¡ng trong má»¥c lá»¥c")
                    self.logger.error("âŒ KhÃ´ng tÃ¬m tháº¥y link chÆ°Æ¡ng trong má»¥c lá»¥c")
                    continue

                # Kiá»ƒm tra kiá»ƒu dá»¯ liá»‡u cá»§a links
                if not isinstance(links, (list, tuple)):
                    print(f"âŒ Links khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng: {type(links)}")
                    self.logger.error(f"âŒ Links khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng: {type(links)}")
                    continue

                print(f"ï¿½ TÃ¬m tháº¥y {len(links)} chÆ°Æ¡ng trong má»¥c lá»¥c")
                self.logger.info(f"ğŸ“– TÃ¬m tháº¥y {len(links)} chÆ°Æ¡ng trong má»¥c lá»¥c")

                # Xá»­ lÃ½ start_chapter tá»« config
                config_start_chapter = series.get('start_chapter', 1)
                max_chapters = series.get('max_chapters', None)
                delay = self.settings.get('delay_between_requests', 3)
                current_volume = None
                
                # XÃ¡c Ä‘á»‹nh file mode: náº¿u start_chapter > 1 thÃ¬ append, ngÆ°á»£c láº¡i ghi Ä‘Ã¨
                file_mode = 'a' if config_start_chapter > 1 else 'w'
                
                if config_start_chapter > 1:
                    print(f"ğŸ¯ Báº¯t Ä‘áº§u tá»« chapter {config_start_chapter} (theo config) - Mode: APPEND")
                    self.logger.info(f"ğŸ¯ Báº¯t Ä‘áº§u tá»« chapter {config_start_chapter} (theo config) - Mode: APPEND")
                else:
                    print(f"ğŸ¯ Báº¯t Ä‘áº§u tá»« chapter {config_start_chapter} - Mode: OVERWRITE")
                    self.logger.info(f"ğŸ¯ Báº¯t Ä‘áº§u tá»« chapter {config_start_chapter} - Mode: OVERWRITE")

                # TÃ­nh start_index tá»« config_start_chapter
                start_index = config_start_chapter - 1  # Chuyá»ƒn tá»« chapter number sang array index
                
                # TÃ­nh end_index
                if max_chapters is None:
                    end_index = len(links)
                else:
                    # max_chapters lÃ  tá»•ng sá»‘ chapters muá»‘n crawl (tÃ­nh tá»« Ä‘áº§u)
                    # Náº¿u start_chapter = 501, max_chapters = 600 -> crawl tá»« 501 Ä‘áº¿n 600
                    end_index = min(len(links), max_chapters)

                print(f"ğŸ¯ Sáº½ crawl tá»« index {start_index} Ä‘áº¿n {end_index-1} (tá»•ng {end_index-start_index} chapters)")
                self.logger.info(f"ğŸ¯ Sáº½ crawl tá»« index {start_index} Ä‘áº¿n {end_index-1} (tá»•ng {end_index-start_index} chapters)")

                try:
                    with open(output_file, file_mode, encoding='utf-8') as f:
                        if file_mode == 'w':
                            f.write(f"=== {series['name']} ===\n\n")

                        for idx in range(start_index, end_index):
                            link_data = links[idx]

                            # Xá»­ lÃ½ cáº£ dict (tá»« JSON) vÃ  string (tá»« parser thÆ°á»ng)
                            if isinstance(link_data, dict):
                                urls = link_data.get('urls', [link_data.get('url')])  # Support multiple URLs
                                chapter_num = link_data.get('chapter_num', idx + 1)
                                chapter_title = link_data.get('title', '')
                            else:
                                urls = [link_data]  # String URL
                                chapter_num = idx + 1
                                chapter_title = ''

                            # Sá»­ dá»¥ng chapter_num tá»« JSON mapping, fallback vá» index + 1
                            actual_chapter_num = chapter_num if chapter_num is not None else (idx + 1)
                            chapter_info = f"Chapter {actual_chapter_num}"
                            print(f"ğŸ“– Crawl {chapter_info} ({len(urls)} URLs): {chapter_title}")
                            self.logger.info(f"ğŸ“– Crawl {chapter_info} ({len(urls)} URLs): {chapter_title}")

                            # Crawl táº¥t cáº£ URLs vÃ  merge content
                            merged_content = []
                            merged_title = ""
                            merged_volume = ""

                            for url_idx, url in enumerate(urls):
                                print(f"  ğŸ“„ Crawl URL {url_idx + 1}/{len(urls)}: {url}")
                                self.logger.info(f"  ğŸ“„ Crawl URL {url_idx + 1}/{len(urls)}: {url}")

                                result = self.crawl_with_retry(url)
                                if not result:
                                    warn_msg = f"âš ï¸  Bá» qua URL {url_idx + 1} cá»§a {chapter_info}"
                                    print(warn_msg)
                                    self.logger.warning(warn_msg)
                                    continue

                                title = result.get('title', '').strip()
                                volume = result.get('volume', '').strip()
                                content = result.get('content', '').strip()

                                # Láº¥y title vÃ  volume tá»« URL Ä‘áº§u tiÃªn
                                if url_idx == 0:
                                    merged_title = title
                                    merged_volume = volume

                                # Merge content
                                if content:
                                    if url_idx == 0:
                                        # Main content: chá»‰ append content, khÃ´ng thÃªm title (Ä‘Ã£ cÃ³ "ChÆ°Æ¡ng X:")
                                        merged_content.append(content)
                                    else:
                                        # Sub content: thÃªm title (vÃ¬ title lÃ  ná»™i dung) + content
                                        if title:
                                            merged_content.append(f"{title}\n\n{content}")
                                        else:
                                            merged_content.append(content)

                                # Delay giá»¯a cÃ¡c URLs
                                if url_idx < len(urls) - 1:
                                    time.sleep(1)  # Delay ngáº¯n giá»¯a URLs cá»§a cÃ¹ng chapter

                            # Xá»­ lÃ½ merged content
                            final_content = '\n\n'.join(merged_content) if merged_content else ''

                            if not final_content:
                                warn_msg = f"âš ï¸  Bá» qua {chapter_info} do khÃ´ng cÃ³ content"
                                print(warn_msg)
                                self.logger.warning(warn_msg)
                                continue

                            self.logger.info(f"ğŸ“ Ghi {chapter_info}: {merged_title[:50]}... (merged tá»« {len(urls)} URLs)")

                            output_lines = []
                            if merged_volume and merged_volume != current_volume:
                                current_volume = merged_volume
                                output_lines.append(f"\n{current_volume}:")

                            forced_title = f"{chapter_info}: {merged_title}" if merged_title else f"{chapter_info}:"
                            output_lines.append(f"\n{forced_title}")

                            if final_content:
                                parser_cls_for_clean = self.get_parser_by_type(parser_type)  # DÃ¹ng parser tá»« config
                                if parser_cls_for_clean:
                                    # Táº¡o instance cá»§a parser Ä‘á»ƒ gá»i method
                                    parser_instance_for_clean = parser_cls_for_clean()
                                    clean_method = getattr(parser_instance_for_clean, 'clean_content', None)
                                    if clean_method and callable(clean_method):
                                        clean_content = clean_method(final_content)
                                    else:
                                        clean_content = final_content
                                else:
                                    clean_content = final_content
                                output_lines.append(clean_content)
                                output_lines.append("")

                            f.write('\n'.join(output_lines))
                            f.flush()

                            print(f"â³ Äá»£i {delay} giÃ¢y...")
                            time.sleep(delay)

                        completion_msg = f"ğŸ‰ HoÃ n thÃ nh {series['name']} theo má»¥c lá»¥c: {end_index} chapters"
                        print(completion_msg)
                        self.logger.info(completion_msg)

                except Exception as e:
                    error_msg = f"âŒ Lá»—i khi Ä‘ang crawl (catalog) vÃ  ghi file '{output_file}': {e}"
                    print(error_msg)
                    self.logger.error(error_msg)

        except KeyboardInterrupt:
            interrupt_msg = "\nâš ï¸  NgÆ°á»i dÃ¹ng dá»«ng crawler"
            print(interrupt_msg)
            self.logger.warning(interrupt_msg)
        except Exception as e:
            critical_msg = f"âŒ Lá»—i nghiÃªm trá»ng: {e}"
            print(critical_msg)
            self.logger.critical(critical_msg)
        finally:
            self.logger.info("ğŸ”’ ÄÃ³ng browser")
            self.close_browser()

    def crawl_series_to_yaml(self, series):
        """Crawl series vÃ  xuáº¥t trá»±c tiáº¿p ra YAML format vá»›i sorting"""
        try:
            print(f"\nğŸš€ Báº¯t Ä‘áº§u crawl series: {series['name']}")
            self.logger.info(f"ğŸš€ Báº¯t Ä‘áº§u crawl series: {series['name']}")
            
            # Setup parser
            parser_type = series.get('parser', 'tw')
            parser_cls = self.get_parser_by_type(parser_type)
            if not parser_cls:
                error_msg = f"âŒ Parser '{parser_type}' khÃ´ng Ä‘Æ°á»£c há»— trá»£"
                print(error_msg)
                self.logger.error(error_msg)
                return False
            
            self.current_parser = parser_cls
            
            # Setup output file
            output_dir = self.settings.get('output_dir', 'output')
            os.makedirs(output_dir, exist_ok=True)
            
            # Táº¡o tÃªn file YAML
            safe_name = re.sub(r'[^\w\-_\.]', '_', series['name'])
            output_file = os.path.join(output_dir, f"{safe_name}.yaml")
            
            # JSON-only approach: táº¥t cáº£ parsers Ä‘á»u dÃ¹ng JSON mapping
            json_mapping = series.get('json_mapping')
            
            if not json_mapping:
                print("âŒ Thiáº¿u json_mapping trong series config")
                self.logger.error("âŒ Thiáº¿u json_mapping trong series config")
                return False
            
            print(f"ğŸ“‹ Sá»­ dá»¥ng JSON mapping cho parser {parser_type}: {json_mapping}")
            self.logger.info(f"ğŸ“‹ Sá»­ dá»¥ng JSON mapping cho parser {parser_type}: {json_mapping}")
            
            # Táº¡o instance cá»§a parser Ä‘á»ƒ gá»i method
            parser_instance = parser_cls()
            
            # JSON-only: chá»‰ dÃ¹ng get_catalog_links_from_config
            enhanced_method = getattr(parser_instance, 'get_catalog_links_from_config', None)
            if enhanced_method and callable(enhanced_method):
                links = enhanced_method(self.page, "", series)  # catalog_url khÃ´ng cáº§n thiáº¿t
            else:
                print("âŒ Parser khÃ´ng cÃ³ method get_catalog_links_from_config")
                self.logger.error("âŒ Parser khÃ´ng cÃ³ method get_catalog_links_from_config")
                return False
            
            if not links:
                error_msg = f"âŒ KhÃ´ng tÃ¬m tháº¥y link chÆ°Æ¡ng trong JSON mapping"
                print(error_msg)
                self.logger.error(error_msg)
                return False
            
            print(f"âœ… TÃ¬m tháº¥y {len(links)} chapters trong JSON mapping")
            self.logger.info(f"âœ… TÃ¬m tháº¥y {len(links)} chapters trong JSON mapping")
            
            # Crawl settings
            delay = series.get('delay', self.settings.get('delay', 2))
            max_chapters = series.get('max_chapters')
            start_chapter = series.get('start_chapter', 1)
            
            # TÃ­nh toÃ¡n range
            start_index = start_chapter - 1  # Chuyá»ƒn tá»« chapter number sang array index
            
            if max_chapters is None:
                end_index = len(links)
            else:
                # max_chapters lÃ  tá»•ng sá»‘ chapters muá»‘n crawl (tÃ­nh tá»« Ä‘áº§u)
                end_index = min(len(links), max_chapters)
            
            # Warning cho YAML mode náº¿u resume
            if start_chapter > 1 and os.path.exists(output_file):
                print(f"âš ï¸  YAML mode: File {output_file} Ä‘Ã£ tá»“n táº¡i vÃ  sáº½ bá»‹ GHI ÄÃˆ")
                print(f"âš ï¸  YAML khÃ´ng há»— trá»£ append. Náº¿u muá»‘n giá»¯ data cÅ©, hÃ£y backup file trÆ°á»›c!")
                self.logger.warning(f"YAML mode: File {output_file} sáº½ bá»‹ ghi Ä‘Ã¨ (khÃ´ng há»— trá»£ append)")
            
            print(f"ğŸ“Š Sáº½ crawl tá»« index {start_index} Ä‘áº¿n {end_index-1} (chapters {start_chapter} Ä‘áº¿n {end_index})")
            self.logger.info(f"ğŸ“Š Sáº½ crawl tá»« index {start_index} Ä‘áº¿n {end_index-1} (chapters {start_chapter} Ä‘áº¿n {end_index})")
            
            # Collect all chapters data
            chapters_data = []
            
            for idx in range(start_index, end_index):
                link_data = links[idx]
                
                # Xá»­ lÃ½ cáº£ dict (tá»« JSON) vÃ  string (tá»« parser thÆ°á»ng)
                if isinstance(link_data, dict):
                    urls = link_data.get('urls', [link_data.get('url')])
                    chapter_num = link_data.get('chapter_num', idx + 1)
                    chapter_title = link_data.get('title', '')
                else:
                    urls = [link_data]
                    chapter_num = idx + 1
                    chapter_title = ''
                
                actual_chapter_num = chapter_num if chapter_num is not None else (idx + 1)
                chapter_info = f"Chapter {actual_chapter_num}"
                print(f"ğŸ“– Crawl {chapter_info} ({len(urls)} URLs): {chapter_title}")
                
                # Crawl táº¥t cáº£ URLs vÃ  merge content
                merged_content = []
                merged_title = ""
                merged_volume = ""
                
                for url_idx, url in enumerate(urls):
                    print(f"  ğŸ“„ Crawl URL {url_idx + 1}/{len(urls)}: {url}")
                    
                    result = self.crawl_with_retry(url)
                    if not result:
                        print(f"âš ï¸  Bá» qua URL {url_idx + 1} cá»§a {chapter_info}")
                        continue
                    
                    title = result.get('title', '').strip()
                    volume = result.get('volume', '').strip()
                    content = result.get('content', '').strip()
                    
                    # Láº¥y title vÃ  volume tá»« URL Ä‘áº§u tiÃªn
                    if url_idx == 0:
                        merged_title = title
                        merged_volume = volume
                    
                    # Merge content
                    if content:
                        if url_idx == 0:
                            merged_content.append(content)
                        else:
                            if title:
                                merged_content.append(f"{title}\n\n{content}")
                            else:
                                merged_content.append(content)
                    
                    # Delay giá»¯a cÃ¡c URLs
                    if url_idx < len(urls) - 1:
                        time.sleep(1)
                
                # Xá»­ lÃ½ merged content
                final_content = '\n\n'.join(merged_content) if merged_content else ''
                
                if not final_content:
                    print(f"âš ï¸  Bá» qua {chapter_info} do khÃ´ng cÃ³ content")
                    continue
                
                # Clean content
                parser_cls_for_clean = self.get_parser_by_type(parser_type)
                if parser_cls_for_clean:
                    parser_instance_for_clean = parser_cls_for_clean()
                    clean_method = getattr(parser_instance_for_clean, 'clean_content', None)
                    if clean_method and callable(clean_method):
                        clean_content = clean_method(final_content)
                    else:
                        clean_content = final_content
                else:
                    clean_content = final_content
                
                # Táº¡o segment data
                segment_id = f"Chapter_{actual_chapter_num}_Segment_1"
                chapter_data = {
                    "id": segment_id,
                    "title": merged_title or f"Chapter {actual_chapter_num}",
                    "content": clean_content
                }
                
                chapters_data.append({
                    'data': chapter_data,
                    'chapter_num': actual_chapter_num,
                    'volume': merged_volume
                })
                
                print(f"â³ Äá»£i {delay} giÃ¢y...")
                time.sleep(delay)
            
            # SORTING: Sáº¯p xáº¿p chapters theo chapter_num
            chapters_data.sort(key=lambda x: x['chapter_num'])
            
            # Táº¡o YAML segments
            yaml_segments = []
            for chapter_info in chapters_data:
                yaml_segments.append(chapter_info['data'])
            
            # Ghi YAML file
            print(f"ğŸ’¾ Ghi YAML file: {output_file}")
            with open(output_file, 'w', encoding='utf-8') as f:
                yaml.dump(yaml_segments, f, allow_unicode=True, sort_keys=False, default_flow_style=False)
            
            completion_msg = f"ğŸ‰ HoÃ n thÃ nh {series['name']}: {len(yaml_segments)} chapters -> {output_file}"
            print(completion_msg)
            self.logger.info(completion_msg)
            
            return True
            
        except Exception as e:
            error_msg = f"âŒ Lá»—i crawl series '{series['name']}': {e}"
            print(error_msg)
            self.logger.error(error_msg)
            return False

    def run_all_series_yaml(self):
        """Cháº¡y táº¥t cáº£ series vá»›i YAML output format"""
        try:
            self.start_browser()
            
            series_list = self.config.get('series', [])
            if not series_list:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y series nÃ o trong config")
                return
            
            print(f"ğŸ“š TÃ¬m tháº¥y {len(series_list)} series")
            
            for series in series_list:
                if not series.get('enabled', True):
                    print(f"â­ï¸  Bá» qua series '{series['name']}' (disabled)")
                    continue
                
                success = self.crawl_series_to_yaml(series)
                if not success:
                    print(f"âŒ Tháº¥t báº¡i crawl series '{series['name']}'")
                    continue
                
                print(f"âœ… HoÃ n thÃ nh series '{series['name']}'")
                
                # Delay giá»¯a cÃ¡c series
                series_delay = self.settings.get('series_delay', 5)
                if series_delay > 0:
                    print(f"â³ Äá»£i {series_delay} giÃ¢y trÆ°á»›c khi crawl series tiáº¿p theo...")
                    time.sleep(series_delay)
            
            print("ğŸ‰ HoÃ n thÃ nh táº¥t cáº£ series!")
            
        except KeyboardInterrupt:
            print("\nâš ï¸  NgÆ°á»i dÃ¹ng dá»«ng crawler")
        except Exception as e:
            print(f"âŒ Lá»—i nghiÃªm trá»ng: {e}")
        finally:
            self.logger.info("ğŸ”’ ÄÃ³ng browser")
            self.close_browser()


def main():
    """Main function"""
    import sys
    import io
    if sys.stdout.encoding != 'utf-8':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    print("ğŸš€ Unified Novel Crawler")
    print("=" * 60)
    
    # TÃ¬m file config trong nhiá»u vá»‹ trÃ­
    config_files = [
        "config.json",
        "crawler_config.json", 
        "series_config.json"
    ]
    
    # CÃ¡c thÆ° má»¥c Ä‘á»ƒ tÃ¬m config
    search_dirs = [
        os.getcwd(),  # ThÆ° má»¥c hiá»‡n táº¡i
        os.path.dirname(os.path.abspath(__file__)),  # ThÆ° má»¥c chá»©a script
        os.path.join(os.path.dirname(os.path.abspath(__file__)), ".."),  # Parent directory
    ]
    
    found_config = None
    found_dir = None
    
    print(f"ğŸ” TÃ¬m config trong cÃ¡c thÆ° má»¥c:")
    for search_dir in search_dirs:
        print(f"   ğŸ“ {search_dir}")
        for config_file in config_files:
            config_path = os.path.join(search_dir, config_file)
            if os.path.exists(config_path):
                found_config = os.path.abspath(config_path)  # Sá»­ dá»¥ng absolute path
                found_dir = search_dir
                print(f"âœ… TÃ¬m tháº¥y file config: {config_path}")
                break
        if found_config:
            break
    
    if not found_config:
        print(f"\nâŒ KhÃ´ng tÃ¬m tháº¥y config file trong cÃ¡c thÆ° má»¥c Ä‘Ã£ tÃ¬m")
        print(f"ğŸ“ Há»— trá»£ cÃ¡c file: {', '.join(config_files)}")
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y, há»i ngÆ°á»i dÃ¹ng
        config_file = input("Nháº­p path file config (hoáº·c Enter Ä‘á»ƒ thoÃ¡t): ").strip()
        if not config_file:
            print("âŒ Cáº§n file config Ä‘á»ƒ cháº¡y!")
            return
        
        if not os.path.exists(config_file):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y config file: {config_file}")
            return
        
        found_config = os.path.abspath(config_file)
    
    print(f"ğŸ“‹ Sá»­ dá»¥ng config: {found_config}")
    print(f"ğŸ“‚ Working directory: {os.getcwd()}")
    
    # ThÃªm thÆ° má»¥c script vÃ o Python path Ä‘á»ƒ import modules
    script_dir = os.path.dirname(os.path.abspath(__file__))
    if script_dir not in sys.path:
        sys.path.insert(0, script_dir)
    
    crawler = UnifiedCrawler(found_config)
    
    # Há»i ngÆ°á»i dÃ¹ng vá» output format
    print("\nğŸ“‹ Chá»n output format:")
    print("1. TXT (format cÅ©)")
    print("2. YAML (format má»›i vá»›i sorting)")
    
    while True:
        choice = input("Nháº­p lá»±a chá»n (1 hoáº·c 2): ").strip()
        if choice == '1':
            print("ğŸ“ Sá»­ dá»¥ng TXT output format")
            crawler.run_all_series()
            break
        elif choice == '2':
            print("ğŸ“ Sá»­ dá»¥ng YAML output format vá»›i sorting")
            crawler.run_all_series_yaml()
            break
        else:
            print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡! Vui lÃ²ng nháº­p 1 hoáº·c 2.")

if __name__ == "__main__":
    main()