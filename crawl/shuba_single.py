#!/usr/bin/env python3
"""
Shuba Single Crawler - Crawl tá»« chÆ°Æ¡ng Ä‘áº§u tiÃªn
================================================

Crawler Ä‘á»™c láº­p cho 69shuba.com, báº¯t Ä‘áº§u tá»« URL chÆ°Æ¡ng Ä‘áº§u tiÃªn
vÃ  tá»± Ä‘á»™ng crawl theo next_url cho Ä‘áº¿n háº¿t truyá»‡n.
"""

import re
import json
import os
import sys
from playwright.sync_api import sync_playwright
from urllib.parse import urljoin


class ShubaSingleCrawler:
    """Crawler Ä‘á»™c láº­p cho 69shuba.com"""
    
    def __init__(self, output_file="shuba_single_output.txt"):
        self.output_file = output_file
        self.playwright = None
        self.browser = None
        self.page = None
        self.crawled_urls = set()  # Track URLs Ä‘Ã£ crawl Ä‘á»ƒ trÃ¡nh loop
        
    def start_browser(self):
        """Khá»Ÿi Ä‘á»™ng browser Edge"""
        print("ğŸŒ Khá»Ÿi Ä‘á»™ng browser Edge...")
        self.playwright = sync_playwright().start()
        
        # Sá»­ dá»¥ng Edge browser
        self.browser = self.playwright.chromium.launch(
            headless=False,  # Hiá»ƒn thá»‹ browser Ä‘á»ƒ debug
            channel='msedge',
            args=[
                '--no-sandbox',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security'
            ]
        )
        
        self.page = self.browser.new_page()
        
        # Set user agent
        self.page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        })
        
        print("âœ… Browser khá»Ÿi Ä‘á»™ng thÃ nh cÃ´ng")
    
    def close_browser(self):
        """ÄÃ³ng browser"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        print("ğŸ”’ Browser Ä‘Ã£ Ä‘Ã³ng")
    
    def extract_content(self, url):
        """
        Extract content tá»« má»™t trang 69shuba.com
        Sá»­ dá»¥ng logic tá»« shuba_parser.py
        """
        try:
            print(f"ğŸ“– Crawling: {url}")
            
            # Navigate to page
            self.page.goto(url, timeout=30000)
            self.page.wait_for_load_state('networkidle')
            
            # Extract title tá»« h1.hide720 hoáº·c tá»« JavaScript bookinfo
            title = "KhÃ´ng cÃ³ tiÃªu Ä‘á»"
            
            # Thá»­ láº¥y tá»« h1 element
            title_el = self.page.query_selector('h1.hide720')
            if title_el:
                title = title_el.inner_text().strip()
                print(f"  âœ… Title: {title}")
            else:
                # Fallback: láº¥y tá»« JavaScript bookinfo.chaptername
                page_source = self.page.content()
                match = re.search(r'chaptername:\s*[\'"]([^\'"]+)[\'"]', page_source)
                if match:
                    title = match.group(1)
                    print(f"  âœ… Title tá»« JS: {title}")
                else:
                    print(f"  âš ï¸  KhÃ´ng tÃ¬m tháº¥y title!")
            
            # Extract content tá»« div.txtnav
            content = ""
            content_container = self.page.query_selector('div.txtnav')
            if content_container:
                print(f"  âœ… TÃ¬m tháº¥y div.txtnav")
                content_html = content_container.inner_html()
                print(f"  ğŸ“ HTML content length: {len(content_html)} chars")
                
                # Clean HTML content
                content_text = self._clean_html_content(content_html, title)
                content = content_text.strip()
                print(f"  ğŸ“ Cleaned content length: {len(content)} chars")
            else:
                print(f"  âŒ KHÃ”NG tÃ¬m tháº¥y div.txtnav!")
                content = ""
            
            # Extract next URL tá»« JavaScript bookinfo.next_page
            next_url = None
            try:
                page_source = self.page.content()
                match = re.search(r'next_page:\s*[\'"]([^\'"]+)[\'"]', page_source)
                if match:
                    next_page = match.group(1)
                    if next_page and next_page != 'index.html':
                        # Kiá»ƒm tra xem next_page Ä‘Ã£ lÃ  absolute URL chÆ°a
                        if next_page.startswith('http'):
                            next_url = next_page  # ÄÃ£ lÃ  absolute URL
                        else:
                            # Build absolute URL tá»« relative path
                            base_url = '/'.join(url.split('/')[:-1])
                            next_url = f"{base_url}/{next_page}"
                        print(f"  â¡ï¸  Next URL: {next_url}")
                    else:
                        print(f"  ğŸ KhÃ´ng cÃ³ next URL (cÃ³ thá»ƒ lÃ  chapter cuá»‘i)")
            except Exception as e:
                print(f"  âš ï¸  Lá»—i extract next_url: {e}")
                next_url = None
            
            return {
                'title': title,
                'content': content,
                'next_url': next_url,
                'success': bool(content)
            }
            
        except Exception as e:
            print(f"âŒ Lá»—i crawl {url}: {e}")
            return {
                'title': None,
                'content': None,
                'next_url': None,
                'success': False
            }
    
    def _clean_html_content(self, html_content, extracted_title=None):
        """
        Clean HTML content vÃ  convert sang text
        Logic tá»« shuba_parser.py
        """
        if not html_content:
            return ""
        
        # Loáº¡i bá» cÃ¡c pháº§n khÃ´ng cáº§n thiáº¿t
        html_content = re.sub(r'<h1[^>]*class="hide720"[^>]*>.*?</h1>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<div[^>]*class="txtinfo[^"]*"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<div[^>]*id="txtright"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<div[^>]*class="bottom-ad"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<div[^>]*class="contentadv"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        
        # Convert HTML entities
        html_content = html_content.replace('&#8195;&#8195;', '    ')  # Em spaces
        html_content = html_content.replace('<br>', '\n')
        html_content = html_content.replace('<br/>', '\n')
        html_content = html_content.replace('<br />', '\n')
        
        # Loáº¡i bá» cÃ¡c tháº» HTML cÃ²n láº¡i
        html_content = re.sub(r'<[^>]+>', '', html_content)
        
        # Loáº¡i bá» title Ä‘Ã£ extract khá»i content (náº¿u cÃ³)
        if extracted_title:
            lines = html_content.split('\n')
            if lines and lines[0].strip() == extracted_title.strip():
                lines = lines[1:]  # Bá» dÃ²ng Ä‘áº§u tiÃªn
                html_content = '\n'.join(lines)
        
        # Clean up whitespace
        lines = []
        for line in html_content.split('\n'):
            line = line.strip()
            if line:
                lines.append(line)
        
        # ThÃªm dáº¥u ' vÃ o dÃ²ng Ä‘áº§u tiÃªn (sau khi Ä‘Ã£ clean)
        if lines:
            lines[0] = "'" + lines[0]
        
        return '\n\n'.join(lines)
    
    def crawl_from_first_chapter(self, first_url, max_chapters=None):
        """
        Crawl tá»« chÆ°Æ¡ng Ä‘áº§u tiÃªn cho Ä‘áº¿n háº¿t
        
        Args:
            first_url: URL cá»§a chÆ°Æ¡ng Ä‘áº§u tiÃªn
            max_chapters: Giá»›i háº¡n sá»‘ chÆ°Æ¡ng (None = khÃ´ng giá»›i háº¡n)
        """
        print(f"ğŸš€ Báº¯t Ä‘áº§u crawl tá»«: {first_url}")
        print(f"ğŸ“ Output file: {self.output_file}")
        if max_chapters:
            print(f"ğŸ“Š Giá»›i háº¡n: {max_chapters} chÆ°Æ¡ng")
        
        # Khá»Ÿi Ä‘á»™ng browser
        self.start_browser()
        
        try:
            # Táº¡o output file
            with open(self.output_file, 'w', encoding='utf-8') as f:
                f.write("=== Shuba Single Crawler Output ===\n\n")
            
            current_url = first_url
            chapter_count = 0
            
            while current_url and current_url not in self.crawled_urls:
                # Kiá»ƒm tra giá»›i háº¡n
                if max_chapters and chapter_count >= max_chapters:
                    print(f"ğŸ“Š ÄÃ£ Ä‘áº¡t giá»›i háº¡n {max_chapters} chÆ°Æ¡ng")
                    break
                
                # Crawl chapter hiá»‡n táº¡i
                result = self.extract_content(current_url)
                
                if not result['success']:
                    print(f"âŒ KhÃ´ng thá»ƒ crawl: {current_url}")
                    break
                
                # Ghi vÃ o file
                self._write_chapter_to_file(result, chapter_count + 1)
                
                # ÄÃ¡nh dáº¥u Ä‘Ã£ crawl
                self.crawled_urls.add(current_url)
                chapter_count += 1
                
                print(f"âœ… HoÃ n thÃ nh chÆ°Æ¡ng {chapter_count}: {result['title']}")
                
                # Chuyá»ƒn sang chÆ°Æ¡ng tiáº¿p theo
                current_url = result['next_url']
                
                if current_url:
                    print(f"â³ Äá»£i 3 giÃ¢y trÆ°á»›c khi crawl tiáº¿p...")
                    import time
                    time.sleep(3)
            
            print(f"ğŸ‰ HoÃ n thÃ nh crawl: {chapter_count} chÆ°Æ¡ng")
            print(f"ğŸ“ Káº¿t quáº£ lÆ°u táº¡i: {self.output_file}")
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Crawl bá»‹ dá»«ng bá»Ÿi user")
        except Exception as e:
            print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh crawl: {e}")
        finally:
            self.close_browser()
    
    def _write_chapter_to_file(self, result, chapter_num):
        """Ghi chapter vÃ o file output"""
        try:
            with open(self.output_file, 'a', encoding='utf-8') as f:
                f.write(f"Chapter {chapter_num}: {result['title']}\n")
                if result['content']:
                    f.write(f"{result['content']}\n\n")
                else:
                    f.write("(KhÃ´ng cÃ³ ná»™i dung)\n\n")
        except Exception as e:
            print(f"âŒ Lá»—i ghi file: {e}")


def main():
    """Main function"""
    if len(sys.argv) < 2:
        print("Usage: python shuba_single.py <first_chapter_url> [max_chapters] [output_file]")
        print("Example: python shuba_single.py https://www.69shuba.com/txt/85122/39443144 10")
        return
    
    first_url = sys.argv[1]
    max_chapters = int(sys.argv[2]) if len(sys.argv) > 2 else None
    output_file = sys.argv[3] if len(sys.argv) > 3 else "shuba_single_output.txt"
    
    crawler = ShubaSingleCrawler(output_file)
    crawler.crawl_from_first_chapter(first_url, max_chapters)


if __name__ == "__main__":
    main()
