#!/usr/bin/env python3
"""
ShubaParser - Parser cho 69shuba.com
====================================

Parser ch√≠nh cho www.69shuba.com s·ª≠ d·ª•ng Playwright
ƒê·ªçc t·ª´ JSON mapping v√† extract content t·ª´ div.txtnav
"""

import re
import json
import os
import sys
from pathlib import Path
from urllib.parse import urljoin
from .base_parser import StandardParserMixin

# Add dich_cli to path ƒë·ªÉ s·ª≠ d·ª•ng PathHelper
project_root = Path(__file__).parent.parent.parent.parent.parent  # parsers -> crawl -> python -> test -> Dich
sys.path.insert(0, str(project_root / "dich_cli"))
from core.path_helper import get_path_helper  # type: ignore[import]


class ShubaParser(StandardParserMixin):
    """Parser cho www.69shuba.com"""
    
    @staticmethod
    def extract_content(page, current_url):
        """
        Extract content t·ª´ m·ªôt trang www.69shuba.com
        
        Args:
            page: Playwright page object
            current_url: URL hi·ªán t·∫°i
            
        Returns:
            dict: {
                'title': str,
                'volume': str,
                'content': str, 
                'next_url': str,
                'success': bool
            }
        """
        try:
            print(f"  üîç Debug: B·∫Øt ƒë·∫ßu extract content t·ª´ {current_url}")
            
            # Extract title t·ª´ h1.hide720 ho·∫∑c t·ª´ JavaScript bookinfo
            title = "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"
            
            # Th·ª≠ l·∫•y t·ª´ h1 element
            title_el = page.query_selector('h1.hide720')
            if title_el:
                title = title_el.inner_text().strip()
                print(f"  ‚úÖ T√¨m th·∫•y title t·ª´ h1: {title}")
            else:
                # Fallback: l·∫•y t·ª´ JavaScript bookinfo.chaptername
                page_source = page.content()
                match = re.search(r'chaptername:\s*[\'"]([^\'"]+)[\'"]', page_source)
                if match:
                    title = match.group(1)
                    print(f"  ‚úÖ T√¨m th·∫•y title t·ª´ JS: {title}")
                else:
                    print(f"  ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y title!")
            
            
            # Volume kh√¥ng c√≥ tr√™n site n√†y
            volume = ""
            
            # Extract content t·ª´ div.txtnav - l·∫•y to√†n b·ªô tr∆∞·ªõc, sau ƒë√≥ clean
            content = ""
            content_container = page.query_selector('div.txtnav')
            if content_container:
                print(f"  ‚úÖ T√¨m th·∫•y div.txtnav")
                # L·∫•y to√†n b·ªô HTML content c·ªßa div.txtnav
                content_html = content_container.inner_html()
                print(f"  üìè HTML content length: {len(content_html)} chars")
                
                # Clean HTML b·∫±ng c√°ch lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt
                content_text = ShubaParser._clean_html_content(content_html, title)
                content = content_text.strip()
                print(f"  üìù Cleaned content length: {len(content)} chars")
            else:
                print(f"  ‚ùå KH√îNG t√¨m th·∫•y div.txtnav!")
                content = ""
            
            # Extract next URL t·ª´ JavaScript bookinfo.next_page
            next_url = None
            try:
                page_source = page.content()
                match = re.search(r'next_page:\s*[\'"]([^\'"]+)[\'"]', page_source)
                if match:
                    next_page = match.group(1)
                    if next_page and next_page != 'index.html':
                        # Build absolute URL
                        base_url = '/'.join(current_url.split('/')[:-1])
                        next_url = f"{base_url}/{next_page}"
                        print(f"  ‚û°Ô∏è  Next URL: {next_url}")
                    else:
                        print(f"  üèÅ Kh√¥ng c√≥ next URL (c√≥ th·ªÉ l√† chapter cu·ªëi)")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  L·ªói extract next_url: {e}")
                next_url = None
            
            print(f"  üìä K·∫øt qu·∫£ extract:")
            print(f"    - Title: {title}")
            print(f"    - Content length: {len(content)}")
            print(f"    - Next URL: {next_url}")
            print(f"    - Success: {bool(content)}")
            
            return {
                'title': title,
                'volume': volume,
                'content': content,
                'next_url': next_url,
                'success': bool(content)
            }
            
        except Exception as e:
            print(f"‚ùå L·ªói parse content 69shuba: {e}")
            import traceback
            traceback.print_exc()
            return {
                'title': None,
                'volume': None,
                'content': None,
                'next_url': None,
                'success': False
            }
    
    @staticmethod
    def _clean_html_content(html_content, extracted_title=None):
        """Clean HTML content v√† convert sang text"""
        if not html_content:
            return ""
        
        # Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt tr∆∞·ªõc khi parse
        # Lo·∫°i b·ªè h1.hide720 (title)
        html_content = re.sub(r'<h1[^>]*class="hide720"[^>]*>.*?</h1>', '', html_content, flags=re.DOTALL)
        
        # Lo·∫°i b·ªè div.txtinfo (author, date info)
        html_content = re.sub(r'<div[^>]*class="txtinfo[^"]*"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        
        # Lo·∫°i b·ªè div#txtright (ads/scripts)
        html_content = re.sub(r'<div[^>]*id="txtright"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        
        # Lo·∫°i b·ªè div.bottom-ad (ads)
        html_content = re.sub(r'<div[^>]*class="bottom-ad"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        
        # Lo·∫°i b·ªè scripts
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        
        # Lo·∫°i b·ªè c√°c div ads kh√°c
        html_content = re.sub(r'<div[^>]*class="contentadv"[^>]*>.*?</div>', '', html_content, flags=re.DOTALL)
        
        # Convert HTML entities
        html_content = html_content.replace('&#8195;&#8195;', '    ')  # Em spaces
        html_content = html_content.replace('<br>', '\n')
        html_content = html_content.replace('<br/>', '\n')
        html_content = html_content.replace('<br />', '\n')
        
        # Lo·∫°i b·ªè c√°c th·∫ª HTML c√≤n l·∫°i
        html_content = re.sub(r'<[^>]+>', '', html_content)
        
        # Lo·∫°i b·ªè title ƒë√£ extract kh·ªèi content (n·∫øu c√≥)
        if extracted_title:
            # Lo·∫°i b·ªè d√≤ng ƒë·∫ßu ti√™n n·∫øu n√≥ gi·ªëng v·ªõi title ƒë√£ extract
            lines = html_content.split('\n')
            if lines and lines[0].strip() == extracted_title.strip():
                lines = lines[1:]  # B·ªè d√≤ng ƒë·∫ßu ti√™n
                html_content = '\n'.join(lines)
        
        # Clean up whitespace
        lines = []
        for line in html_content.split('\n'):
            line = line.strip()
            if line:
                lines.append(line)
        
        # Th√™m d·∫•u ' v√†o d√≤ng ƒë·∫ßu ti√™n (sau khi ƒë√£ clean)
        if lines:
            lines[0] = "'" + lines[0]
        
        return '\n\n'.join(lines)

    @staticmethod
    def clean_content(content):
        """Clean content, gi·ªØ nguy√™n structure"""
        if not content:
            return content
        
        # Remove zero-width characters
        replacements = {
            '\u200b': '',  # Zero-width space
            '\u200c': '',  # Zero-width non-joiner  
            '\u200d': '',  # Zero-width joiner
            '\ufeff': '',  # Byte order mark
        }
        
        for old, new in replacements.items():
            content = content.replace(old, new)
        
        # Fix excessive line breaks
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
        
        # Normalize Chinese spaces
        content = re.sub(r'„ÄÄ+', '„ÄÄ„ÄÄ', content)
        
        return content.strip()
    
    @staticmethod
    def get_catalog_links_from_config(page, catalog_url, series_config):
        """
        L·∫•y danh s√°ch links t·ª´ JSON mapping
        
        Args:
            page: Playwright page (KH√îNG S·ª¨ D·ª§NG, ch·ªâ ƒë·ªÉ compatible)
            catalog_url: URL c·ªßa trang m·ª•c l·ª•c (kh√¥ng c·∫ßn thi·∫øt n·∫øu d√πng JSON)
            series_config: Dict config c·ªßa series
            
        Returns:
            list: Danh s√°ch chapter URLs ho·∫∑c dicts
        """
        # ∆Øu ti√™n JSON mapping
        json_mapping = series_config.get('json_mapping')
        if json_mapping:
            # S·ª≠ d·ª•ng PathHelper ƒë·ªÉ resolve path (t·ª± ƒë·ªông x·ª≠ l√Ω relative/absolute)
            ph = get_path_helper()
            json_path = ph.resolve(json_mapping)
            
            if not os.path.exists(json_path):
                print(f"  ‚ùå Kh√¥ng t√¨m th·∫•y file JSON: {json_mapping}")
                print(f"     ƒê√£ th·ª≠: {json_path}")
                return []
            
            print(f"  üìñ ƒê·ªçc JSON mapping: {ph.relative_to_project(json_path)}")
            
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                links = []
                for item in data:
                    # Support both single URL and multiple URLs
                    urls = item.get('urls', [item.get('url')])
                    if isinstance(urls, str):
                        urls = [urls]
                    
                    links.append({
                        'chapter_num': item.get('chapter_num'),
                        'title': item.get('title', ''),
                        'url': urls[0] if urls else None,
                        'urls': urls
                    })
                
                print(f"  ‚úÖ ƒê·ªçc ƒë∆∞·ª£c {len(links)} chapters t·ª´ JSON")
                return links
                
            except Exception as e:
                print(f"  ‚ùå L·ªói ƒë·ªçc JSON mapping: {e}")
                return []
        
        print("  ‚ùå Kh√¥ng c√≥ JSON mapping")
        return []