#!/usr/bin/env python3
"""
Workflow Dá»‹ch Thuáº­t sá»­ dá»¥ng Google Gemini Native SDK
MODULE nÃ y Ä‘Æ°á»£c gá»i bá»Ÿi master_workflow.py khi cÃ³ tÃ¹y chá»n.

Cáº¤U HÃŒNH QUAN TRá»ŒNG:
1. thinking_budget = 0: Táº®T thinking (tiáº¿t kiá»‡m token, tá»‘c Ä‘á»™ nhanh)
2. thinking_budget = 1024: Báº­t thinking vá»›i budget cá»‘ Ä‘á»‹nh
3. thinking_budget = -1: Dynamic thinking (model tá»± quyáº¿t Ä‘á»‹nh)
4. Safety settings: ÄÃ£ Táº®T Táº¤T Cáº¢ (BLOCK_NONE)

VÃ Dá»¤ CONFIG:
{
  "translate_api_settings": {
    "api_key": "your_gemini_api_key",
    "model": "gemini-2.5-flash",
    "temperature": 0.7,
    "max_tokens": 4000,
    "thinking_budget": 0,
    "concurrent_requests": 3,
    "delay": 1
  }
}
"""

import os
import sys
import yaml
import time
import queue
import threading
from datetime import datetime

# SDK cá»§a Google - sá»­ dá»¥ng package má»›i
from google import genai
from google.genai import types

# Import cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t tá»« cÃ¡c module khÃ¡c
script_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(script_dir)
from clean_segment import CustomDumper

# --- CÃ¡c hÃ m pricing Ä‘Æ¡n giáº£n ---

def load_pricing_data():
    """Load pricing data tá»« file JSON."""
    pricing_file = os.path.join(script_dir, 'model_pricing.json')
    try:
        with open(pricing_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file pricing: {pricing_file}")
        return None
    except Exception as e:
        print(f"âš ï¸ Lá»—i load pricing data: {e}")
        return None

def calculate_total_cost(token_stats, model_name):
    """TÃ­nh tá»•ng chi phÃ­ dá»±a trÃªn token stats vÃ  model pricing."""
    pricing_data = load_pricing_data()
    if not pricing_data or 'models' not in pricing_data:
        return None
    
    models = pricing_data['models']
    if model_name not in models:
        return None
    
    model_pricing = models[model_name]
    input_tokens = token_stats.get('total_input', 0)
    output_tokens = token_stats.get('total_output', 0)
    thinking_tokens = token_stats.get('total_thinking', 0)
    
    # TÃ­nh chi phÃ­ (giÃ¡ / 1M tokens)
    input_cost = (input_tokens * model_pricing['input_price']) / 1_000_000
    output_cost = (output_tokens * model_pricing['output_price']) / 1_000_000
    thinking_cost = (thinking_tokens * model_pricing['input_price']) / 1_000_000  # Thinking tokens tÃ­nh nhÆ° input
    
    total_cost = input_cost + output_cost + thinking_cost
    
    return {
        'input_cost': input_cost,
        'output_cost': output_cost,
        'thinking_cost': thinking_cost,
        'total_cost': total_cost,
        'currency': pricing_data.get('currency', 'USD')
    }

# --- CÃ¡c hÃ m tiá»‡n Ã­ch chung ---

def load_yaml(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def save_yaml(data, file_path):
    output_dir = os.path.dirname(file_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    with open(file_path, 'w', encoding='utf-8') as f:
        yaml.dump(data, f, allow_unicode=True, sort_keys=False, Dumper=CustomDumper)

def load_prompt(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read().strip()

def write_log(log_file, segment_id, status, error=None, token_info=None):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_message = f"[{timestamp}] {segment_id}: {status}"
    
    # ThÃªm thÃ´ng tin token náº¿u cÃ³
    if token_info:
        input_tokens = token_info.get('input_tokens', 'N/A')
        output_tokens = token_info.get('output_tokens', 'N/A')
        thinking_tokens = token_info.get('thinking_tokens', 'N/A')
        total_tokens = token_info.get('total_tokens', 'N/A')
        
        if thinking_tokens != 'N/A' and thinking_tokens > 0:
            log_message += f" | Tokens: Input={input_tokens}, Output={output_tokens}, Thinking={thinking_tokens}, Total={total_tokens}"
        else:
            log_message += f" | Tokens: Input={input_tokens}, Output={output_tokens}, Total={total_tokens}"
    
    if error:
        log_message += f" - Lá»—i: {error}"
    
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(log_message + "\n")
    print(log_message)

def supports_thinking_model(model_name):
    """Kiá»ƒm tra xem model cÃ³ há»— trá»£ thinking hay khÃ´ng (chá»‰ 2.5 series)."""
    return any(version in model_name.lower() for version in ['2.5', '2-5'])

def clean_content(content):
    """
    LÃ m sáº¡ch vÃ  format láº¡i content tá»« Gemini:
    - ThÃªm 1 dÃ²ng tráº¯ng giá»¯a má»—i dÃ²ng Ä‘á»ƒ dá»… Ä‘á»c (náº¿u chÆ°a cÃ³)
    - Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    - Giá»¯ nguyÃªn format náº¿u Ä‘Ã£ cÃ³ dÃ²ng trá»‘ng sáºµn
    """
    if not content:
        return content
    
    # Kiá»ƒm tra xem content Ä‘Ã£ cÃ³ dÃ²ng trá»‘ng giá»¯a cÃ¡c dÃ²ng chÆ°a
    lines = content.split('\n')
    non_empty_lines = [line.strip() for line in lines if line.strip()]
    
    # Náº¿u sá»‘ dÃ²ng gá»‘c gáº¥p Ä‘Ã´i sá»‘ dÃ²ng cÃ³ ná»™i dung, cÃ³ thá»ƒ Ä‘Ã£ cÃ³ format sáºµn
    if len(lines) >= len(non_empty_lines) * 1.5:
        # ÄÃ£ cÃ³ format tá»‘t, chá»‰ loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        return '\n'.join(line.strip() if line.strip() else "" for line in lines)
    
    # ChÆ°a cÃ³ format, thÃªm dÃ²ng tráº¯ng giá»¯a má»—i dÃ²ng
    formatted_lines = []
    for i, line in enumerate(non_empty_lines):
        formatted_lines.append(line)
        # ThÃªm dÃ²ng tráº¯ng giá»¯a cÃ¡c dÃ²ng (trá»« dÃ²ng cuá»‘i cÃ¹ng)
        if i < len(non_empty_lines) - 1:
            formatted_lines.append("")
    
    return '\n'.join(formatted_lines)

# --- Logic dá»‹ch thuáº­t vá»›i Gemini Native SDK ---

def gemini_worker(q, result_dict, client, model_name, generation_config, system_prompt, log_file, total_segments, lock, delay, token_stats):
    """HÃ m worker cho thread xá»­ lÃ½ dá»‹ch vá»›i Gemini Native SDK."""
    while not q.empty():
        try:
            idx, segment = q.get(block=False)
            segment_id = segment['id']
            with lock:
                current_processed = len([v for v in result_dict.values() if v is not None])
                print(f"\n[{current_processed + 1}/{total_segments}] (Gemini SDK) Äang dá»‹ch {segment_id}...")
            
            try:
                # Káº¿t há»£p system prompt vÃ  user prompt
                full_prompt = f"{system_prompt}\n\nDá»‹ch Ä‘oáº¡n vÄƒn sau tá»« tiáº¿ng Trung sang tiáº¿ng Viá»‡t:\n\n{segment['content']}"
                
                response = client.models.generate_content(
                    model=model_name,
                    contents=full_prompt,
                    config=generation_config
                )
                
                # Kiá»ƒm tra xem prompt cÃ³ bá»‹ cháº·n khÃ´ng trÆ°á»›c khi truy cáº­p káº¿t quáº£
                if not response.candidates:
                    # Láº¥y lÃ½ do bá»‹ cháº·n tá»« prompt_feedback
                    block_reason = "KhÃ´ng rÃµ"
                    if response.prompt_feedback:
                        block_reason = response.prompt_feedback.block_reason.name
                    raise Exception(f"Prompt bá»‹ cháº·n bá»Ÿi bá»™ lá»c an toÃ n: {block_reason}")

                translated = response.text
                
                # Debug: Kiá»ƒm tra content cÃ³ trá»‘ng khÃ´ng
                if not translated or not translated.strip():
                    raise Exception("Model tráº£ vá» content trá»‘ng - cÃ³ thá»ƒ do thinking budget quÃ¡ cao hoáº·c lá»—i API")
                
                # Láº¥y thÃ´ng tin token usage tá»« response
                token_info = None
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    try:
                        input_tokens = response.usage_metadata.prompt_token_count
                        output_tokens = response.usage_metadata.candidates_token_count
                        total_tokens = response.usage_metadata.total_token_count
                        
                        # Láº¥y thinking tokens náº¿u model há»— trá»£ (chá»‰ 2.5 series)
                        thinking_tokens = 0
                        if supports_thinking_model(model_name):
                            thinking_tokens = getattr(response.usage_metadata, 'thoughts_token_count', 0) or 0
                        
                        token_info = {
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'thinking_tokens': thinking_tokens,
                            'total_tokens': total_tokens
                        }
                        
                        # Cáº­p nháº­t token stats chung (thread-safe)
                        with lock:
                            token_stats['total_input'] += input_tokens
                            token_stats['total_output'] += output_tokens
                            token_stats['total_thinking'] += thinking_tokens
                            token_stats['total_overall'] += total_tokens
                            token_stats['request_count'] += 1
                    except AttributeError:
                        # Náº¿u khÃ´ng cÃ³ usage_metadata hoáº·c cáº¥u trÃºc khÃ¡c, bá» qua
                        pass
                
                # LÃ m sáº¡ch vÃ  format láº¡i content
                cleaned_content = clean_content(translated)
                translated_segment = {'id': segment['id'], 'title': segment['title'], 'content': cleaned_content}
                
                with lock:
                    result_dict[idx] = translated_segment
                    write_log(log_file, segment_id, "THÃ€NH CÃ”NG", token_info=token_info)
            
            except Exception as e:
                with lock:
                    result_dict[idx] = segment # Giá»¯ láº¡i segment gá»‘c náº¿u lá»—i
                    write_log(log_file, segment_id, "THáº¤T Báº I", str(e))
            
            q.task_done()
            time.sleep(delay)
        except queue.Empty:
            break

def translate_with_gemini_threading(segments_to_translate, client, model_name, generation_config, system_prompt, config, log_file):
    """HÃ m Ä‘iá»u phá»‘i threading cho Gemini SDK."""
    q = queue.Queue()
    result_dict = {}
    lock = threading.Lock()
    total_segments = len(segments_to_translate)
    
    # Khá»Ÿi táº¡o token statistics
    token_stats = {
        'total_input': 0,
        'total_output': 0,
        'total_thinking': 0,
        'total_overall': 0,
        'request_count': 0
    }
    
    for idx, segment in enumerate(segments_to_translate):
        q.put((idx, segment))
        result_dict[idx] = None
    
    api_config = config['translate_api_settings']
    num_threads = min(api_config.get("concurrent_requests", 5), len(segments_to_translate))
    threads = []
    
    for _ in range(num_threads):
        t = threading.Thread(
            target=gemini_worker,
            args=(
                q, result_dict, client, model_name, generation_config, system_prompt, 
                log_file, total_segments, lock,
                api_config.get("delay", 1),
                token_stats
            )
        )
        t.daemon = True
        t.start()
        threads.append(t)
    
    for t in threads:
        t.join()
    
    results = []
    for idx in sorted(result_dict.keys()):
        if result_dict[idx] is not None:
            results.append(result_dict[idx])
    
    return results, token_stats

def gemini_native_workflow(master_config):
    """
    Workflow chÃ­nh cho viá»‡c dá»‹ch báº±ng Gemini Native SDK.
    """
    print("\n" + "="*20 + " Báº®T Äáº¦U WORKFLOW GEMINI NATIVE SDK " + "="*20)
    
    api_config = master_config['translate_api_settings']
    paths = master_config['paths']
    active_task = master_config['active_task']

    input_file = active_task.get('source_yaml_file')
    system_prompt_file = paths.get('prompt_file')

    # --- Sá»­a Ä‘á»•i logic Ä‘áº·t tÃªn file Ä‘á»ƒ nháº¥t quÃ¡n ---
    # 1. Táº¡o base name vÃ  timestamp
    base_name = os.path.splitext(os.path.basename(input_file))[0]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 2. Táº¡o tÃªn file output vÃ  log nháº¥t quÃ¡n
    # VÃ­ dá»¥: tanaka_cleaned_gemini_native_20250623_174711
    output_basename_with_timestamp = f"{base_name}_cleaned_gemini_native_{timestamp}"
    
    final_output_file = os.path.join(paths['output_dir'], f"{output_basename_with_timestamp}.yaml")
    log_file = os.path.join(paths['log_dir'], f"{output_basename_with_timestamp}.log")

    # Äáº£m báº£o thÆ° má»¥c tá»“n táº¡i
    if not os.path.exists(paths['output_dir']):
        os.makedirs(paths['output_dir'])
    if not os.path.exists(paths['log_dir']):
        os.makedirs(paths['log_dir'])
    # --- Káº¿t thÃºc sá»­a Ä‘á»•i ---

    # --- Khá»Ÿi táº¡o Gemini Client ---
    try:
        client = genai.Client(api_key=api_config["api_key"])
        
        # Cáº¥u hÃ¬nh an toÃ n - Táº®T Táº¤T Cáº¢
        safety_settings = [
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, 
                threshold=types.HarmBlockThreshold.OFF
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, 
                threshold=types.HarmBlockThreshold.OFF
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, 
                threshold=types.HarmBlockThreshold.OFF
            ),
            types.SafetySetting(
                category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, 
                threshold=types.HarmBlockThreshold.OFF
            ),
        ]
        
        # Cáº¥u hÃ¬nh sinh ná»™i dung
        generation_config_params = {
            "temperature": api_config['temperature'],
            "max_output_tokens": api_config.get('max_tokens', 4000)
        }
        
        # Kiá»ƒm tra xem model cÃ³ há»— trá»£ thinking khÃ´ng (chá»‰ 2.5 series)
        if supports_thinking_model(api_config['model']):
            # ThÃªm thinking_config - máº·c Ä‘á»‹nh lÃ  0 (disable thinking) Ä‘á»ƒ tiáº¿t kiá»‡m token
            thinking_budget = api_config.get('thinking_budget', 0)  # Máº·c Ä‘á»‹nh = 0
            if thinking_budget is not None:
                try:
                    # Theo documentation má»›i cá»§a Gemini API
                    generation_config_params['thinking_config'] = types.ThinkingConfig(
                        thinking_budget=int(thinking_budget)
                    )
                    if thinking_budget == 0:
                        print(f"ğŸ’¡ (Gemini SDK) ÄÃ£ Táº®T thinking (thinking_budget = 0) Ä‘á»ƒ tiáº¿t kiá»‡m token")
                    else:
                        print(f"ğŸ’¡ (Gemini SDK) ÄÃ£ Ã¡p dá»¥ng thinking_budget: {thinking_budget}")
                except Exception as e:
                    print(f"âš ï¸ (Gemini SDK) Lá»—i khi Ã¡p dá»¥ng thinking_budget: {e}. Bá» qua...")
        else:
            print(f"â„¹ï¸ (Gemini SDK) Model {api_config['model']} khÃ´ng há»— trá»£ thinking - bá» qua thinking_config")

        generation_config = types.GenerateContentConfig(**generation_config_params, safety_settings=safety_settings)
        
        print(f"âœ… (Gemini SDK) ÄÃ£ khá»Ÿi táº¡o thÃ nh cÃ´ng client vÃ  config cho model: {api_config['model']}")
    except Exception as e:
        print(f"âŒ (Gemini SDK) Lá»—i khá»Ÿi táº¡o client: {e}")
        return

    system_prompt = load_prompt(system_prompt_file)
    segments_to_translate = load_yaml(input_file)
    total_segments = len(segments_to_translate)

    # Ghi log ban Ä‘áº§u
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"--- Báº®T Äáº¦U GEMINI NATIVE WORKFLOW {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
        f.write(f"TÃ¡c vá»¥: {active_task.get('task_name', 'KhÃ´ng tÃªn')}\n")
        f.write(f"Input: {input_file}\nOutput: {final_output_file}\nModel: {api_config['model']}\n\n")

    print(f"Báº¯t Ä‘áº§u dá»‹ch {total_segments} segment vá»›i {api_config['concurrent_requests']} threads...")

    translated_segments, token_stats = translate_with_gemini_threading(
        segments_to_translate, client, api_config['model'], generation_config, system_prompt, master_config, log_file
    )
    
    if not translated_segments:
        print("\nâŒ Dá»‹ch thuáº­t tháº¥t báº¡i, khÃ´ng cÃ³ segment nÃ o Ä‘Æ°á»£c tráº£ vá». Dá»«ng workflow.")
        return

    # LÆ°u káº¿t quáº£ cuá»‘i cÃ¹ng (bá» qua bÆ°á»›c dá»n dáº¹p vÃ¬ workflow nÃ y chá»‰ dá»‹ch)
    save_yaml(translated_segments, final_output_file)

    # TÃ­nh toÃ¡n chi phÃ­ dá»± kiáº¿n
    cost_info = calculate_total_cost(token_stats, api_config['model'])
    
    # Ghi token summary vÃ  cost vÃ o log
    model_supports_thinking = supports_thinking_model(api_config['model'])
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n--- TOKEN USAGE SUMMARY ---\n")
        f.write(f"Tá»•ng sá»‘ request thÃ nh cÃ´ng: {token_stats['request_count']}\n")
        f.write(f"Tá»•ng Input tokens: {token_stats['total_input']:,}\n")
        f.write(f"Tá»•ng Output tokens: {token_stats['total_output']:,}\n")
        if model_supports_thinking:
            f.write(f"Tá»•ng Thinking tokens: {token_stats['total_thinking']:,}\n")
        f.write(f"Tá»•ng tokens sá»­ dá»¥ng: {token_stats['total_overall']:,}\n")
        if token_stats['request_count'] > 0:
            avg_input = token_stats['total_input'] / token_stats['request_count']
            avg_output = token_stats['total_output'] / token_stats['request_count']
            f.write(f"Trung bÃ¬nh Input tokens/request: {avg_input:.1f}\n")
            f.write(f"Trung bÃ¬nh Output tokens/request: {avg_output:.1f}\n")
            if model_supports_thinking:
                avg_thinking = token_stats['total_thinking'] / token_stats['request_count']
                f.write(f"Trung bÃ¬nh Thinking tokens/request: {avg_thinking:.1f}\n")
        
        # Ghi thÃ´ng tin chi phÃ­
        f.write(f"\n--- CHI PHÃ Dá»° KIáº¾N ---\n")
        if cost_info:
            f.write(f"Chi phÃ­ Input tokens: ${cost_info['input_cost']:.6f}\n")
            f.write(f"Chi phÃ­ Output tokens: ${cost_info['output_cost']:.6f}\n")
            if cost_info['thinking_cost'] > 0:
                f.write(f"Chi phÃ­ Thinking tokens: ${cost_info['thinking_cost']:.6f}\n")
            f.write(f"Tá»”NG CHI PHÃ Dá»° KIáº¾N: ${cost_info['total_cost']:.6f} {cost_info['currency']}\n")
        else:
            f.write(f"KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin giÃ¡ cáº£ cho model: {api_config['model']}\n")
        
        f.write(f"\n--- Káº¾T THÃšC WORKFLOW {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
    
    print("\nğŸ‰ (Gemini SDK) Dá»ŠCH THUáº¬T HOÃ€N Táº¤T! ğŸ‰")
    print(f"Káº¿t quáº£ cuá»‘i cÃ¹ng: {final_output_file}")
    print(f"Log chi tiáº¿t: {log_file}")
    
    # Hiá»ƒn thá»‹ token summary
    print(f"\nğŸ“Š TOKEN USAGE SUMMARY:")
    print(f"â”œâ”€ Sá»‘ request thÃ nh cÃ´ng: {token_stats['request_count']}")
    print(f"â”œâ”€ Tá»•ng Input tokens: {token_stats['total_input']:,}")
    print(f"â”œâ”€ Tá»•ng Output tokens: {token_stats['total_output']:,}")
    if model_supports_thinking:
        print(f"â”œâ”€ Tá»•ng Thinking tokens: {token_stats['total_thinking']:,}")
    print(f"â””â”€ Tá»•ng tokens sá»­ dá»¥ng: {token_stats['total_overall']:,}")
    if token_stats['request_count'] > 0:
        avg_input = token_stats['total_input'] / token_stats['request_count']
        avg_output = token_stats['total_output'] / token_stats['request_count']
        if model_supports_thinking and token_stats['total_thinking'] > 0:
            avg_thinking = token_stats['total_thinking'] / token_stats['request_count']
            print(f"   Trung bÃ¬nh: {avg_input:.1f} input + {avg_output:.1f} output + {avg_thinking:.1f} thinking = {avg_input + avg_output + avg_thinking:.1f} tokens/request")
        else:
            print(f"   Trung bÃ¬nh: {avg_input:.1f} input + {avg_output:.1f} output = {avg_input + avg_output:.1f} tokens/request")
    
    # Hiá»ƒn thá»‹ thÃ´ng tin chi phÃ­
    print(f"\nğŸ’° CHI PHÃ Dá»° KIáº¾N:")
    if cost_info:
        print(f"â”œâ”€ Chi phÃ­ Input: ${cost_info['input_cost']:.6f}")
        print(f"â”œâ”€ Chi phÃ­ Output: ${cost_info['output_cost']:.6f}")
        if cost_info['thinking_cost'] > 0:
            print(f"â”œâ”€ Chi phÃ­ Thinking: ${cost_info['thinking_cost']:.6f}")
        print(f"â””â”€ ğŸ’µ Tá»”NG: ${cost_info['total_cost']:.6f} {cost_info['currency']}")
    else:
        print(f"â””â”€ âš ï¸ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin giÃ¡ cáº£ cho model: {api_config['model']}") 